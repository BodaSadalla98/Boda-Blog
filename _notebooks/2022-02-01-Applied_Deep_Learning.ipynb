{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908c9a47",
   "metadata": {},
   "source": [
    "# Applied Deep Learning\n",
    "> Applied Deep Learning [Course](https://github.com/maziarraissi/Applied-Deep-Learning) \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae636b1",
   "metadata": {},
   "source": [
    "## Deep Learning overview\n",
    "* we can look at deep learning as an algorithm that writes algorithms, like a compiler\n",
    " - in this case the source code would be the data: (examples/experiences)\n",
    " - excutable code would be the deployable model \n",
    "\n",
    " * Deep: Functions compositions  $ f_l f_{l-1} .... f_1$\n",
    " * Learning: Loss, Back-propagation, and Gradient Descent\n",
    "\n",
    " * $ L(\\theta) \\approx J(\\theta)$ --> noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent \n",
    " * why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow \n",
    " ### Optimizers\n",
    " * to make gradient descent faster, we can add momentum to it.\n",
    "  * another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fe29",
   "metadata": {},
   "source": [
    "  * RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. \n",
    "\n",
    "  ![](assets/Applied_deep_learning/rprop.png)\n",
    "\n",
    "\n",
    "  * Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e06d5f",
   "metadata": {},
   "source": [
    "* Adam:\n",
    "    - can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)\n",
    "    - can also has momentum for all parameter wich can lead to faster convergence\n",
    "* Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51897a05",
   "metadata": {},
   "source": [
    "## ImageNet:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53571c3",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "* A simple method to prevent the NN from overfitting \n",
    "* CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image \n",
    "* you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82030f",
   "metadata": {},
   "source": [
    "### Network In Network\n",
    "* the main idea is to put a network inside another network\n",
    "\n",
    "* they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers\n",
    "* this idea is bisacally a (one to one convution) \n",
    "* they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0739a3",
   "metadata": {},
   "source": [
    "* one by one convolution is a normal convolution with fliter size of 1 by 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5aec",
   "metadata": {},
   "source": [
    "### VGG Net\n",
    "\n",
    "#### Local Response Normalization:\n",
    "* the idea is to normalize a pixel acros nearing channels "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
