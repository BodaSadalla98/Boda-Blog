{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908c9a47",
   "metadata": {},
   "source": [
    "# Applied Deep Learning\n",
    "> Applied Deep Learning [Course](https://github.com/maziarraissi/Applied-Deep-Learning) \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae636b1",
   "metadata": {},
   "source": [
    "## Deep Learning overview\n",
    "* we can look at deep learning as an algorithm that writes algorithms, like a compiler\n",
    " - in this case the source code would be the data: (examples/experiences)\n",
    " - excutable code would be the deployable model \n",
    "\n",
    " * Deep: Functions compositions  $ f_l f_{l-1} .... f_1$\n",
    " * Learning: Loss, Back-propagation, and Gradient Descent\n",
    "\n",
    " * $ L(\\theta) \\approx J(\\theta)$ --> noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent \n",
    " * why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow \n",
    " ### Optimizers\n",
    " * to make gradient descent faster, we can add momentum to it.\n",
    "  * another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fe29",
   "metadata": {},
   "source": [
    "  * RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. \n",
    "\n",
    "  ![](assets/Applied_deep_learning/rprop.png)\n",
    "\n",
    "\n",
    "  * Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e06d5f",
   "metadata": {},
   "source": [
    "* Adam:\n",
    "    - can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)\n",
    "    - can also has momentum for all parameter wich can lead to faster convergence\n",
    "* Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53571c3",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "* A simple method to prevent the NN from overfitting \n",
    "* CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image \n",
    "* you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79537fbe",
   "metadata": {},
   "source": [
    "# Computer Vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67da4c",
   "metadata": {},
   "source": [
    "## Image Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18a38c",
   "metadata": {},
   "source": [
    "### Large Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82030f",
   "metadata": {},
   "source": [
    "#### Network In Network\n",
    "* the main idea is to put a network inside another network\n",
    "\n",
    "* they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers\n",
    "* this idea is bisacally a (one to one convution) \n",
    "* they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0739a3",
   "metadata": {},
   "source": [
    "* one by one convolution is a normal convolution with fliter size of 1 by 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b1273",
   "metadata": {},
   "source": [
    "* in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had  slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant)\n",
    "* we can achieve local invariant with pooling, and deal with global invariant with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5aec",
   "metadata": {},
   "source": [
    "\n",
    "#### VGG Net\n",
    "\n",
    "##### Local Response Normalization:\n",
    "* the idea is to normalize a pixel across nearing channels \n",
    "\n",
    "* after comparing nets with lrn and nets without, they didn't find big difference, so they stoped using it \n",
    "\n",
    "##### Data Augmentation\n",
    "* Image translations( random crops), and horizontal reflection \n",
    "* altering the intensities of the RGB channels \n",
    "* scale jittering   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c030d81",
   "metadata": {},
   "source": [
    "#### GoogleNet\n",
    "\n",
    "* You stack multiple inception modules on top of each ohter \n",
    "* the idea is that you don't have to choose which filter size to use, so why don't use them all \n",
    "* to make the network more efficient, they first projected the input with one by one convolution then applied the main filters \n",
    "* you concatinate the many filters through the channel dimension    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab15387",
   "metadata": {},
   "source": [
    "#### Batch Normalization \n",
    "\n",
    "* The main goal of batch normalization is to redude the `Internal Covariant Shift`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f025de2",
   "metadata": {},
   "source": [
    "* we can just normalize the inputs and it would work fine\n",
    "* the problem is that in each following layer, and statistics of its output would depend on its weights \n",
    "* so we also need to nomalize the inputs in hidden layers \n",
    "* here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen\n",
    "\n",
    "* in inference we can't have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset \n",
    "\n",
    "##### conv layers\n",
    "* for conv layers we apply normalization across every channel for every pixel in the batch of images\n",
    "* the effective bach size would be ==> m*p*q where m is the number of images in the batch and \n",
    "p,q are the image resolution \n",
    "\n",
    "##### Benifits of batch norm:\n",
    "* you can use higher learning rate, as the training is more stable \n",
    "* less sensitive to initialization \n",
    "* less sensitive to activation function \n",
    "* it has regularization effects, because thre's random mini batch every time \n",
    "* preserve gradient magintude ?? maybe --> because the jacobian doesn't scale as we scales the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a127c4",
   "metadata": {},
   "source": [
    "#### Parametric Relu:\n",
    "\n",
    "$ f({y_i}) = \\max(0,y_i) + a_i \\min(0, y_i) $\n",
    "* if $a_i = 0$  --> Relu\n",
    "* if $a_i = 0.01$ --> Leaky Relu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceafe0",
   "metadata": {},
   "source": [
    "* the initialization of weights and biases depends on the type of activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ceec6",
   "metadata": {},
   "source": [
    "#### Kaiming Initialization  (I didn't fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations):\n",
    "\n",
    "- professor went into deep mathematical details into how to choose the intial values for weights\n",
    "* the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we  want the weights to have values centred around 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127625d2",
   "metadata": {},
   "source": [
    "#### Label smoothing regularization\n",
    "- the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f5b57",
   "metadata": {},
   "source": [
    "#### ResNet \n",
    "\n",
    "* The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection.\n",
    "\n",
    "##### Identity mapping in resnets \n",
    "\n",
    "* the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03f0d3",
   "metadata": {},
   "source": [
    "#### Wide Residual Networks \n",
    "\n",
    "* an attempt to make resnets wider and study if that would make them better "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf26fea",
   "metadata": {},
   "source": [
    "#### ResNext \n",
    "* just like resnets but they changed bottleneck blocks with group convolution block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebea09",
   "metadata": {},
   "source": [
    "#### Squeeze-and-Ecxcitation Networks \n",
    "\n",
    "##### Squeeze : just a global averaging step \n",
    "##### Excitation: is just a fully connected newtwork \n",
    "##### Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention \n",
    "* scaling is you paying different attention to different channels like attention models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04e8e1",
   "metadata": {},
   "source": [
    "#### Spatial Transformer Network \n",
    "* the main idea is to seperate the main object in the image, like putting a box around it and then this box can be resized, shifted, rotated. so in the end we have a focused image that has only the object, and so we can apply convolution on it and it would be easy then \n",
    "\n",
    "* the idea is to first find a good transformation parameters theta, you can do that using NN\n",
    "* then for every position in the output image, you do a bilinear sampling from the input image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82973f30",
   "metadata": {},
   "source": [
    "#### Dynamic Routing between capsuls \n",
    "\n",
    "* the idea is to make the outputs of the capsule has a norm that is the probability that an object is presenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09096219",
   "metadata": {},
   "source": [
    "### Small Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae43e15",
   "metadata": {},
   "source": [
    "#### Knowledge Distillation\n",
    "\n",
    "* the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter `T` that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba8bbb",
   "metadata": {},
   "source": [
    "#### Network Pruning:\n",
    "*  all connections with weights below a threshold are removed from the network \n",
    "* weight are sparse now \n",
    "* then we can represent them using fewer bits\n",
    "\n",
    "#### Quantization\n",
    "* we basically cluster our weight to some centroids\n",
    "* the number of centroids for conv layers are more than the ones for FC layers why:\n",
    "    -   because conv layer filters are already sparse, we need higher level of accuracy in them\n",
    "    -   FC layers are so dense that we can tolerate fewer quantization levels \n",
    "\n",
    "#### Huffman Coding\n",
    "\n",
    "* store the more common symbols with more bits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44692d3a",
   "metadata": {},
   "source": [
    "####  Squeeze Net\n",
    "* the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made \n",
    "* the main idea  is to use one by one comvultion to reduce the dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c55aa",
   "metadata": {},
   "source": [
    "#### XNOR-NET\n",
    "\n",
    "* the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation\n",
    "* the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==> $W = \\alpha * B $ where alpha  is postative 32 bit constant and B is a binary matrix \n",
    "* then mean we try to train by using a means square error loss function of the original weights and alpha and B \n",
    "\n",
    "* I still can't fully understand  how to binarize the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248f32e",
   "metadata": {},
   "source": [
    "#### Mobile Nets\n",
    "\n",
    "* the idea is to reduce computation complexity by doing c   onv for each channel separately, and not across channels.\n",
    "* so we use number of filters as the same as the input channels\n",
    "* but then we will end up with  output size as the input size, so we still need to do one by one convolution to output the correct size\n",
    "\n",
    "#### Xception\n",
    "* unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size\n",
    "\n",
    "#### Mobile Net V2\n",
    "* the same as MobileNet, but with Residuals connections.\n",
    "\n",
    "#### ShuffleNet\n",
    "\n",
    "* the idea is to shuffle channels after doing a group convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114d034",
   "metadata": {},
   "source": [
    "### Auto ML\n",
    "\n",
    "* the question is can we automate architicture engineering, as we automated feature engineering in DL?\n",
    "* we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf3caa",
   "metadata": {},
   "source": [
    "\n",
    "#### Regularized Evolution\n",
    "* it's basically random search + selection\n",
    "* at first you randomly choose some  architecture  train, and eval on it and push it to to the population\n",
    "* then you sample some arch. from the population\n",
    "* then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples \n",
    "* then remove the oldest arch. in the population\n",
    "* you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4f8d",
   "metadata": {},
   "source": [
    "#### EfficientNet \n",
    "\n",
    "* the idea is that we do grid seach on a small network to come with the best depth scaling coefficient `d`, width scaling coefficient `w`, and resolution scalling coefficient `r`, then we try to find scaling parameter $\\phi$, that gives the best accuracy while maintaning the `flops` under the limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aedb8a",
   "metadata": {},
   "source": [
    "### Robustness \n",
    "\n",
    "* The main goal is to make your network robust against adverarial attacks\n",
    "\n",
    "#### Intrigiong peroperties of neural networks \n",
    "* there's nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters \n",
    "* neural networks  has blind spots, this  means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image  \n",
    "* Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets \n",
    "\n",
    "* you can train your network to defend against attacks but that's expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time.\n",
    "\n",
    "* small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality\n",
    "#### untargeted adversiral examples\n",
    "* fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example \n",
    "* then you can just add a weighted loss, one for the  orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples \n",
    "\n",
    "#### Towards Evaluating the Robustness of Neural Networks\n",
    "* another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. \n",
    "\n",
    "![](assets/Applied_deep_learning/adversiral-attack-algo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa88c77",
   "metadata": {},
   "source": [
    "### Visualizing & Understanding\n",
    "* now we want to debug our network, to understand how it works\n",
    "* so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input.\n",
    "* so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass.\n",
    "* we call these max locations, switches\n",
    "* the main idea is, visualising the feature maps, gonna help you modify the network \n",
    "* you can have two models that have the same output for the same input but which one do you trust more?\n",
    "    - to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy \n",
    "#### LIME: Local Interpretable Model-agnsortic Explanations\n",
    "* you want to make trust the model, meaning that you wanna maek sure the model parioritized the important features\n",
    "* but you can't interprete non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model parioritrized \n",
    "\n",
    "#### Understanding Deep Learning Requires Rethinking Generalization\n",
    "* NN are powerful enough to fit random data, but then it will not generalize for test data \n",
    "* so when we indroduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection.\n",
    "* so, this means: The model architecture itself isn't a sufficient regularizer.\n",
    "\n",
    "* Explicit regularization: dropout, weight decay, data augmentation\n",
    "* Implicit regularization: early stopping\n",
    "* there exist a two-layer NN with Relu activation, that can fit any N random example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f6e37",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "* labled data is expensive\n",
    "* you split a data set in half,we find that transfer learning for the same task, have higher acc'\n",
    "* transfer learning with fine-tuned weight is better than locking the learned weights\n",
    "* on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task\n",
    "#### DeCAF\n",
    "* first layers learn low-level features, whereas latter layers learn semantic or high-lebel features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867f7ff",
   "metadata": {},
   "source": [
    "## Image Transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6427b",
   "metadata": {},
   "source": [
    "### Semantic Segmentation\n",
    "* you want to segments different classes in the image\n",
    "* The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions.\n",
    "\n",
    "#### Atrous Convolution:\n",
    "* you don't wanna lose much info when you do conv,  and then upsample again, so you fill your filter with holes, so that you lose less info\n",
    "* reduce the degree of signal downsampling \n",
    "#### CRF: \n",
    "* deals with the reduced localization accuracy due to the Deep Convolution NN invariance \n",
    "\n",
    "#### Dilated Convolution:\n",
    "* basically atrous convolution\n",
    "* increases the  size of the receptive points layer by layer\n",
    "\n",
    "### Image Super-Resolution \n",
    "* we want to develope a NN that can up-sample images\n",
    "* we can do that using convolution \n",
    "* and in the middle we use one to one convolution to work as non-leaner mapping\n",
    "\n",
    "### Perceptual Losses \n",
    "* mse isn't the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same \n",
    "* the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images\n",
    "* so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers\n",
    "#### Single Image Super-Resolution(SISR)\n",
    "* the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3211165",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "### Two Stage Detectors \n",
    "\n",
    "#### R-CNN\n",
    "* we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called \"selective search\"\n",
    "* we then enter that to a CNN to do features extractions\n",
    "* at the end we have a per-class classifier \n",
    "\n",
    "#### Spatial Pyramid Pooling \n",
    "* the idea is that we use spatial pyramid pooling to have a fixed length representation for the image\n",
    "* also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image\n",
    "\n",
    "#### Fast R-CNN\n",
    "* just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don't have to calculate many classifiers, one for each class.\n",
    "* also we don't need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map.\n",
    "* last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. \n",
    "#### Feature Pyramids\n",
    "* the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes.\n",
    "* to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows\n",
    "* the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image\n",
    "* to overcome this, from each layer we add a connection to the layer below\n",
    "* so we up sample the feature maps first then do one by one convolution to adjust the number of channels,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1892e1f",
   "metadata": {},
   "source": [
    "### One Stage Detectors \n",
    "#### YOLO \n",
    "* we want the detector to be realtime, so we can detect objects live\n",
    "* divide the input image into  S * S grid\n",
    "* if the center of an object fell inside a cell, that cell is the one responsible to detect that object \n",
    "* each grid cell gonna predict, B bounding boxes, each with confidence score\n",
    "\n",
    "#### SSD: Single Shot MultiBox Detector \n",
    "* we want to take the speed from YOLO, and the high acc from the two-stage detectors\n",
    "* unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO\n",
    "#### YOLO9000 - YOLO V2\n",
    "* Tries to improve upon YOLOv1 using idead from fast-CNN and SSD \n",
    "* we can use higher res images in training\n",
    "* can the anchor boxes from the training images not just randomly \n",
    "* introduced passthrough layer\n",
    "* used hierarchical classification to extend many classes "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
