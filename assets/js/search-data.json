{
  
    
        "post0": {
            "title": "Univnet",
            "content": "TTS (Text To Speech) . TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. . Wavenet . Wavenet V1 . before wavenet, ther was two methods: . generative method: which would produce the over all song of the sentece well, but would fail to produce the individual sounds well | . | concatinative: . we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence | . wavenet: . tries to do both of the above methods, it also can change the speaker by changing some parameters | . | data output: 16 khz rate . | we cant use normal RNN as the max seq length around 50 . | they used dilated CNNs: . can have very long look back | fast to train | . | . WaveNet: is a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals.WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. . Wavenet V2 . The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn&#39;t depend on previous samples to produce the current sample, while still maintaining the same quality. . WaveGan . WaveGAN is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms). . The WaveGAN architecture is based off DCGAN. The DCGAN generator uses the transposed convolution operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed convolution operation to widen its receptive field, using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5, and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way, using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters, numerical operations, and output dimensionality as DCGAN . Before WaveGan . Autoregressive generation: . It&#39;s an approach in which speech samples are generated one by one in sequence. | Examples: WaveNet | Has high quality | Takes around 180 secs to generate a one second of speech | can&#39;t be applied to services in production due to low speed | . Non-autoregressive generation: . It&#39;s an approach where all voice samples are generated in prallel | Examples: Prallel WaveNet | Lower quality than autoregressive method | takes 0.03 seconds to generates one second of speed | . End-to-End TTS . End-to-end TTS systems can be splitted into two main components: . Speech Synthesizer, which takes in raw text and output mel-spectrogram. Ex: Tacotron | . | Vocoder, which takes in mel-spectrogram and outputs sound waves. Ex: Prallel WaveGan, Univnet | . | . Tacotron2 . Tacotron 2 is a neural network architecture for speech synthesis directly from text. It consists of two components: a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. . . Prallel WaveGan . Parallel WaveGAN1, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system. . . Univnet . UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multiresolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input . . Resources . Wavenet https://deepmind.com/blog/article/high-fidelity-speech-synthesis-wavenet | https://www.youtube.com/watch?v=YyUXG-BfDbE | https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html | https://deepmind.com/blog/article/wavenet-generative-model-raw-audio | . | WaveGan https://arxiv.org/pdf/1802.04208v3.pdf | https://paperswithcode.com/method/wavegan | . | Prallel WaveGan . https://www.youtube.com/watch?v=knzT7M6qsl0 | https://github.com/kan-bayashi/ParallelWaveGAN | https://arxiv.org/pdf/1910.11480.pdf | . | Tacotron . https://arxiv.org/pdf/1712.05884v2.pdf | . | Univnet https://arxiv.org/pdf/2106.07889.pdf | . | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/tts/2022/07/01/Univnet.html",
            "relUrl": "/jupyter/deeplearning/python/tts/2022/07/01/Univnet.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Programming Facts",
            "content": "Intro . this is intended to be some kind or reference to go through whenever I face some kind of bug, or error, that I don&#39;t know how to solve.usually these kinds of error that doesn&#39;t make sense, or we don&#39;t know the cause of them, and the worst it, that we don&#39;t find many ppl facing the same issue, so internet can&#39;t be so useful then. | I learned theses facts the hard way, spending so much time trying to figure out the root of the issue. | . Debugging Facts . here are some tips, to help whenever you are facing some error, trying get a package to work, or you these kinds of error that takes you life few days, you know. | . Package installation issues . Python version . this might sound trivial, but every package version, only works with some python versions | so you should run pip install package-name== to get the package versions supported by your python version | . Update Pip . whenever there&#39;s a dependency conflict, or versions conflict, and you can&#39;t install a package, then check your pip and update it if possible | I spent like 4 days clueless why a simple package that I installed a week before, won&#39;t install now, like out of a sudden, it won&#39;t install anymore, due to dependency packages conflict | when I upgraded pip, it simply worked | . Check for broken installations . many times we would try to install some something, then for whatever reason, it wouldn&#39;t complete successful. But then we would try to install again, and we would encounter strange errors, that we don&#39;t know the root for them. | there error could be because there&#39;s a broken installation, that messed things up. | so, when we delete, or remove this broken installation, and install again, it just works. | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/life/python/programming/2022/05/23/programming-facts.html",
            "relUrl": "/jupyter/life/python/programming/2022/05/23/programming-facts.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning Papers Summarization",
            "content": "Decoupled Neural Interfaces using Synthetic Gradients . In NN, the training process, has 3 bottle-necks forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass | backward pass: the same, but for backward propagation | weights lock: you can&#39;t update weights unless you do for weights in next layer | . | the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone | it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer | this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer | . Synthetic Gradient Model . can be just a simple NN that is trained to output the gradient of the layer | it can be trained using the true gradient, or even the synthetic gradient of the next layer | it&#39;s important that the last layer computes the true gradient, as in the end we must have a ground truth to can calculate a true loss, and the NN would actually train | . we can have also synthetic model for forward pass, that works with the same idea | . A Roadmap for Big Models . We are in the Era of Big Models | Model generalization is hard, models trained on certain data domain, doesn&#39;t scare to other | Datasets creation, and high research tasks, made it hard for small companies to train task-specific models | Big models solve thees issues. | . Big Models . Big-data driven | Multi-task Adaptive | can fine-tuned with few-shot learning #### Data issues | data bias | data duplication | data has to cover all domains | low quality data | hard to create huge datasets | . Knowledge . a new way to represent data | we represent knowledge as knowledge graphs | KG consists of: Instances, Relation, Concept, and Values | KG can be created using : experts, wiki-based knowledge graphs, or extracted from unstructured texts #### KG Completion and Integration | most of the known KGs has many fields empty, and there&#39;s a going research in how to deal with that and fill the gaps. | some methods try to do that using intra-graph knowledge augmentation or with inter-graph. | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/04/22/papers.html",
            "relUrl": "/jupyter/deeplearning/python/2022/04/22/papers.html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "GIT",
            "content": "Beatiful commands . git log --oneline --decorate --all --graph . git merge --abort ==&gt; abort merge, and get back like it never happened . git reset --hard ==&gt; is your way to lose all uncommited work in your working directory . git fast forward is basically that git moves the commit pointer upward to the new posotion, without creating a merge commit or anything | you can merge with --no-ff flag, to disable the fast forward merge and force git to create the merge commit | . Git Bisect . used when something broke, and you know what did broke, but you can&#39;t figure out when did it broke | you just give it a testing criteria to test the commit history against | . Methodology . everything inside git is an object | all your local branches are located in .git/refs/heads | a branch is basically a file that appoints to a commit. a branch is bisacally a pointer to specific commit | every commit has a parent, so to assemble branches we follow and compute their parents | . Commits . keep added changes in commits related to the same topic | add informative commit message | you can add parts of changes in a single file using -p flag in git add -p filename0 | . Branching . Long-running branches . Main branch | Dev branch ### Short-lived branches | features branches | bug fixes branches | . Merging . When the one of the two branches has the head is the same as the common ancesstor of the two branches, then we can do a fast-forward merge by putting the commits of the another branch on top the common ancesstor commit | . Rebase . rebase puts the commits of the second brach on top of the common ancesstor commit then rebase the commits of the first branch on top of the last commit of the first branch, then it changes the history of commits | . Only use rebase to clean local commit history, don&#39;t use rebase on commits that is pushed to online .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/git/version-control/2022/03/15/git.html",
            "relUrl": "/jupyter/git/version-control/2022/03/15/git.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Mind Exercise",
            "content": "Sad Thoughts . I think this would be the first post, of random thoughts than cross my mind. Some of these thoughts are sad, and some are with good vibes, but the sad ones are more to be honest. I don&#39;t know why, that&#39;s the case, maybe cause lately for the past few years I&#39;ve been feeling lonely. It&#39;s becoming a constant feeling, that would just lurks in the shadows while im busy with something, but it wouldn&#39;t hesitate to jump out as it gets the chance. Idk why im capturing this, but I believe it in the hope that it would make me feel better, at least for a while hopefully. Secondly in the hope, that someday I would see this post again, and then I would be in a better state, that I would remember these days and thank god appreciating his good to me, I would remember this someday, using the memory to give me boost then, to really believe that all sadness and loneliness would pass eventually. . Ah, btw, im writing this at sunset, at the blue lagoon, Dahab, one of the best relaxing destination in Egypt, and still all these thoughts came to my head. Is it because I look around and see happy people, that are enjoying their time? Many of them are couples, or a large group of friends. Don&#39;t take me wrong, I don&#39;t envy them, I just do these stupid social comparisons that most brains do! So, I would be torn apart between my desires and my conscious, doesn&#39;t matter who wins, I would be the one suffering in the middle. . Enjoy the trip vs thinking about What I could be doing . Whenever I travel, going for vacation, or camping, hiking, etc, you I find young guys just like my age, just standing there all the time in the security check points, working as security, or just standing there doing nothing. Wouldn&#39;t they prefer that they start their live, start make a family? Couldn&#39;t I be one them, doing what they are doing, hating my life every minute? I was lucky, I got rejected, but they weren&#39;t. Is our future and life just depends on luck, or let me say, on the opinion of a doctor working on the military who decides I go in, or I get rejected? These ppl spend on average one and half year in the military, this period is enough to flip their live. Is it fair that im passing by them, so our paths cross, but I&#39;m on the path of having fun, while they are there cursing their life every moment. You know what else? Me or them can&#39;t talk about these feelings, it&#39;s like we can communicate it through looks, but our lips shall not move, for us to see another dawn. . Permenant Mindset Vs Temporary Circumstances . Life doesn&#39;t have to be hard all the time, but u need to be prepared on how to deal with hard times. U are walking in ur way, someplaces gonna be hard and some gonna be easy, buy anyway u continue walking, focusing on the way, are u going on the right track or not, are u going with good speed or not? What about ur acceleration? Are u accelerating or decelerating? Ideally, the environment around you should have as little effect as possible on your pace… . Life Marathon . Life is like a race, or u can say a marathon. And your actions and doings are you speed. Your good or evil intentions and doings are ur compass, ur map. If u think about life with those two principles, then u find that: . It doesn&#39;t matter at age we die, we still reach the end of that marathon, cause god doesn&#39;t reward or punish us for how long we obeyed him or sinned, but we are hold acountable for our intentions. This means a guy that died at 20 yrs old, can have higher reward in the afterlife, than someone who died in his 90s. This can also help explain, the idea that a non-believer would stay in punishment for ever in the afterlife, as disbelief in the one true god can has an infinite weight in the afterlife&#39;s balance of judgment. . The second principle means, that some ppl would be marching in this marathon on the wrong direction and some are going in the right one. So you can find urself at the end at the wrong end line, one that has hell punishment after it. It&#39;s not all black and sad, because acc god gave us an implanted compass inside of each one of us, compass that helps us go to the right track in case we are on the wrong one, and it&#39;s the same compass that also keeps us on the right track encouraging us to continue and to increase our velocity on it. This compass is the natural instinct that god gave every and each one of us when were born. . Atleast be a Sine. Never be a Random Curve . TALK ABOUT THE ONE STRAING ROAD AND WE SHOULD BE GOING ON SIN WAVES WITH HIGH FREQUENCY . Sadness . The constant feelings of sadness is so strong, it&#39;s strong that if in a random moment I feel happy, I would immediately check if im in a rare random dream. It&#39;s sad, that im not try to that feeling anymore that I try to jinx it, and get back to the normal, lost living soul on earth. What makes me feel more sad and angry at myself, is that I don&#39;t know why I feel this way, what&#39;s the source for it all? Could it be that im not religious as I used to be? Could it be my sins that are trying to cover me? Could it just be in my mind? Could it be the tough situations im living it? Could it be the injustices that happen all around me, both inside and abroad? Well, these are the tough questions, that I have to find answers to myself. Yes, that&#39;s right, myself, im the only one who can help me, who else can fully understand my situation, how I feel. Even if I try to explain to a genuine friend who is trying to help, but how can he fully realize it! What if it&#39;s a combination of all these things? What if there&#39;s a solution, but I got used to the constant sadness, that it became my normal, that I don&#39;t want to change it? I keep saying that I can&#39;t continue like this, that I should try to change it, but do I really wanna change it, it&#39;s just a feeling driven by the society standards that it&#39;s not normal to keep feeling this way. . Be an increasing exponential not a mere gaussian . We can model our progress as one of two: . Gaussian Like Progress: . Just like the below Photo, we can observe our progress with exponential growth in the beginning. Then, we would be just starting, and we would be filled with energy. Then, we do a lot of efforts and end up burnt out, and go down again. . . Slow stable increase . The other model, would be small increase that keeps increasing by time, till we reach our saturation point, then we keep at high level or productivity. . Prophet Mohammed(PBUH) said: The best deeds are those which are stable, even if they are small . . I will . I will not envy ppl around me, I will not come with some excuses, they are many of them, it&#39;s easy to stay where you are envying ppl, coming with excusing while u sitting there don&#39;t nothing moving backward, if you aren&#39;t moving forward, then you are moving back, life isn&#39;t stationary | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/life/python/2022/03/13/mind_exercise.html",
            "relUrl": "/jupyter/life/python/2022/03/13/mind_exercise.html",
            "date": " • Mar 13, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Stanford CS224N NLP with Deep Learning",
            "content": "Lecture 1 .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/nlp/2022/03/08/CS224N.html",
            "relUrl": "/jupyter/deeplearning/python/nlp/2022/03/08/CS224N.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Applied Deep Learning",
            "content": "Reference . Repo with the slides, and course info . Deep Learning overview . we can look at deep learning as an algorithm that writes algorithms, like a compiler . in this case the source code would be the data: (examples/experiences) | excutable code would be the deployable model . | Deep: Functions compositions $ f_l f_{l-1} .... f_1$ . | Learning: Loss, Back-propagation, and Gradient Descent . | $ L( theta) approx J( theta)$ --&gt; noisy estimate of the objective function due to mini-batching. That&#39;s why we call it stochastic Gradient Descent . | why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it&#39;s N*N, so it&#39;s computationally expensive and slow ### Optimizers | to make gradient descent faster, we can add momentum to it. | another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum | . | . RMSprop: A mini-batch version of rprop method. the original rprop can&#39;t work with mini batches, as it doesn&#39;t consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. . . | . Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there | . Adam: can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta) | can also has momentum for all parameter wich can lead to faster convergence | . | Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal | . Dropout . A simple method to prevent the NN from overfitting | CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image | you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. | . Computer Vision . Image Classification . Large Networks . Network In Network . the main idea is to put a network inside another network . | they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers . | this idea is bisacally a (one to one convution) | they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels from the last conv layer to form the output layer | . one by one convolution is a normal convolution with fliter size of 1 by 1 | . in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant) | we can achieve local invariant with pooling, and deal with global invariant with data augmentation | . VGG Net . Local Response Normalization: . the idea is to normalize a pixel across nearing channels . | after comparing nets with lrn and nets without, they didn&#39;t find big difference, so they stoped using it . | . Data Augmentation . Image translations( random crops), and horizontal reflection | altering the intensities of the RGB channels | scale jittering | . GoogleNet . You stack multiple inception modules on top of each ohter | the idea is that you don&#39;t have to choose which filter size to use, so why don&#39;t use them all | to make the network more efficient, they first projected the input with one by one convolution then applied the main filters | you concatinate the many filters through the channel dimension | . Batch Normalization . The main goal of batch normalization is to redude the Internal Covariant Shift | . we can just normalize the inputs and it would work fine | the problem is that in each following layer, and statistics of its output would depend on its weights | so we also need to nomalize the inputs in hidden layers | here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen . | in inference we can&#39;t have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset . | . conv layers . for conv layers we apply normalization across every channel for every pixel in the batch of images | the effective bach size would be ==&gt; mpq where m is the number of images in the batch and p,q are the image resolution | . Benifits of batch norm: . you can use higher learning rate, as the training is more stable | less sensitive to initialization | less sensitive to activation function | it has regularization effects, because thre&#39;s random mini batch every time | preserve gradient magintude ?? maybe --&gt; because the jacobian doesn&#39;t scale as we scales the weights | . Parametric Relu: . $ f({y_i}) = max(0,y_i) + a_i min(0, y_i) $ . if $a_i = 0$ --&gt; Relu | if $a_i = 0.01$ --&gt; Leaky Relu | . the initialization of weights and biases depends on the type of activation function | . Kaiming Initialization (I didn&#39;t fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations): . professor went into deep mathematical details into how to choose the intial values for weights | the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we want the weights to have values centred around 1 | . Label smoothing regularization . the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset | . ResNet . The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection. | . Identity mapping in resnets . the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes | . Wide Residual Networks . an attempt to make resnets wider and study if that would make them better | . ResNext . just like resnets but they changed bottleneck blocks with group convolution block | . Squeeze-and-Ecxcitation Networks . Squeeze : just a global averaging step . Excitation: is just a fully connected newtwork . Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention . scaling is you paying different attention to different channels like attention models | . Spatial Transformer Network . the main idea is to seperate the main object in the image, like putting a box around it and then this box can be resized, shifted, rotated. so in the end we have a focused image that has only the object, and so we can apply convolution on it and it would be easy then . | the idea is to first find a good transformation parameters theta, you can do that using NN . | then for every position in the output image, you do a bilinear sampling from the input image | . Dynamic Routing between capsuls . the idea is to make the outputs of the capsule has a norm that is the probability that an object is presenet | . Small Networks . Knowledge Distillation . the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter T that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference. | . Network Pruning: . all connections with weights below a threshold are removed from the network | weight are sparse now | then we can represent them using fewer bits | . Quantization . we basically cluster our weight to some centroids | the number of centroids for conv layers are more than the ones for FC layers why: because conv layer filters are already sparse, we need higher level of accuracy in them | FC layers are so dense that we can tolerate fewer quantization levels | . | . Huffman Coding . store the more common symbols with more bits | . Squeeze Net . the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made | the main idea is to use one by one comvultion to reduce the dimensionality | . XNOR-NET . the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation | the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==&gt; $W = alpha * B $ where alpha is postative 32 bit constant and B is a binary matrix | then mean we try to train by using a means square error loss function of the original weights and alpha and B . | I still can&#39;t fully understand how to binarize the input . | . Mobile Nets . the idea is to reduce computation complexity by doing c onv for each channel separately, and not across channels. | so we use number of filters as the same as the input channels | but then we will end up with output size as the input size, so we still need to do one by one convolution to output the correct size | . Xception . unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size | . Mobile Net V2 . the same as MobileNet, but with Residuals connections. | . ShuffleNet . the idea is to shuffle channels after doing a group convolution | . Auto ML . the question is can we automate architicture engineering, as we automated feature engineering in DL? | we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN | . Regularized Evolution . it&#39;s basically random search + selection | at first you randomly choose some architecture train, and eval on it and push it to to the population | then you sample some arch. from the population | then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples | then remove the oldest arch. in the population | you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch. | . EfficientNet . the idea is that we do grid seach on a small network to come with the best depth scaling coefficient d, width scaling coefficient w, and resolution scalling coefficient r, then we try to find scaling parameter $ phi$, that gives the best accuracy while maintaning the flops under the limit | . Robustness . The main goal is to make your network robust against adverarial attacks | . Intrigiong peroperties of neural networks . there&#39;s nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters | neural networks has blind spots, this means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image | Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets . | you can train your network to defend against attacks but that&#39;s expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time. . | small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality #### untargeted adversiral examples . | fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example | then you can just add a weighted loss, one for the orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples | . Towards Evaluating the Robustness of Neural Networks . another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. | . . Visualizing &amp; Understanding . now we want to debug our network, to understand how it works | so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input. | so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass. | we call these max locations, switches | the main idea is, visualising the feature maps, gonna help you modify the network | you can have two models that have the same output for the same input but which one do you trust more? to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy #### LIME: Local Interpretable Model-agnsortic Explanations | . | you want to trust the model, meaning that you wanna make sure the model prioritized the important features | but you can&#39;t interpret non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model prioritized | . Understanding Deep Learning Requires Rethinking Generalization . NN are powerful enough to fit random data, but then it will not generalize for test data | so when we introduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection. | so, this means: The model architecture itself isn&#39;t a sufficient regularizer. . | Explicit regularization: dropout, weight decay, data augmentation . | Implicit regularization: early stopping | there exist a two-layer NN with Relu activation, that can fit any N random example | . Transfer Learning . labled data is expensive | you split a data set in half,we find that transfer learning for the same task, have higher acc&#39; | transfer learning with fine-tuned weight is better than locking the learned weights | on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task #### DeCAF | first layers learn low-level features, whereas latter layers learn semantic or high-lebel features | . Image Transformation . Semantic Segmentation . you want to segments different classes in the image | The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions. | . Atrous Convolution: . you don&#39;t wanna lose much info when you do conv, and then upsample again, so you fill your filter with holes, so that you lose less info | reduce the degree of signal downsampling #### CRF: | deals with the reduced localization accuracy due to the Deep Convolution NN invariance | . Dilated Convolution: . basically atrous convolution | increases the size of the receptive points layer by layer | . Image Super-Resolution . we want to develope a NN that can up-sample images | we can do that using convolution | and in the middle we use one to one convolution to work as non-leaner mapping | . Perceptual Losses . mse isn&#39;t the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same | the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images | so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers #### Single Image Super-Resolution(SISR) | the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images | . Object Detection . Two Stage Detectors . R-CNN . we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called &quot;selective search&quot; | we then enter that to a CNN to do features extractions | at the end we have a per-class classifier | . Spatial Pyramid Pooling . the idea is that we use spatial pyramid pooling to have a fixed length representation for the image | also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image | . Fast R-CNN . just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don&#39;t have to calculate many classifiers, one for each class. | also we don&#39;t need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map. | last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. #### Feature Pyramids | the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes. | to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows | the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image | to overcome this, from each layer we add a connection to the layer below | so we up sample the feature maps first then do one by one convolution to adjust the number of channels, | . One Stage Detectors . YOLO . we want the detector to be realtime, so we can detect objects live | divide the input image into S * S grid | if the center of an object fell inside a cell, that cell is the one responsible to detect that object | each grid cell gonna predict, B bounding boxes, each with confidence score | . SSD: Single Shot MultiBox Detector . we want to take the speed from YOLO, and the high acc from the two-stage detectors | unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO #### YOLO9000 - YOLO V2 | Tries to improve upon YOLOv1 using idead from fast-CNN and SSD | we can use higher res images in training | can the anchor boxes from the training images not just randomly | introduced passthrough layer | used hierarchical classification to extend many classes | . Video . Large-scale video classification . we would have a context stream which learns features on a low res frames. | and a Fovea Stream, which operates on a high res middle portion of the frames | . Early Fusion . the idea is to take a few frames from the middle of the clip, and apply conv on them, the only diff is that we add a new dimension to filters which coreespond to the number of frames | that&#39;s just for the first layer, but then it&#39;s normal conv #### Late Fusion | we have two separate single-frame networks, each one takes a diff frame from the clip, and we concatenate them in the end | . Two-Stream CNN for action recognition . video can be decomposed into spatial and temporal components | . Optical flow stacking . we can just follow pixels from frame to another, and then create a flow vectors, in the x,y axis | then we can stack these flow vectors #### Trajectory stacking | follow the point from frame to another | . Non-local Neural Network . the idea is that we want to see for output pixel, which areas did it pay attention to in the input | so we attention every output with all possible pixels in the input | if we are using it with videos, then we add another dimension for the time | . Group Normalization . Batch norm, is good as long as we have reasonable batch size | but whe we have very small batch size, then batch norm isn&#39;t the best | . Here&#39;s diff between normalizaiton methods: . . Natural Language Processing . Word Representation . Distributed Representation of Words and Phrases and their Compositionality . Word2Vec ( Efficient Estimation of Word Representation in Vector Space ) . using CBOW model, or skip-gram model | uses the cosine-similarity distance function | . Skip-gram Model . Continuous Bag of Words is the opposite of the skip-gram in the sense of what are we predicting (word vs context) . | the idea is that you pick a word, and try to predict the context around it . | so you have a word in the middle and try to predict words around in (before, and after), given a defined window size | and our objective is to maximize the liklihood of the context given the reference word | we can use binary trees, to do an approximation, and speed up the softmax caculation, as for every word in the vocab, we would calculate it&#39;s softmax with all other words,but now we can use binary trees, and do that in just log(n), using an approximation, that we group words together, and in each level coming from the root, we go right or left, till we reach the word in the leaves | we can make this even faster, using huffman encoding to assign shorter paths for more frequent words . | noise sampling: the idea is to give the model negative samples, that doesn&#39;t appear together, and give it low probability . | . Evaluation . we can evaluate the model, using syntactic, and semantic analogies | for example, Berlin to Germany is like France to Paris | . GloVe: Global Vectors for Word Representation . the idea is to use: global matrix factorization methods | local context window methods | . | we compute a co-occurrence method, that holds the counts every two words come after each other, we try to learn two matrieces and two biases, that log(X) = w1 * w2 + b1 + b2 | . Text Classification . Recursive deep models for semantic compositionality over sentiment Treebank . the dataset is presented as a tree with leafs as words | the idea is that for evert word we get it&#39;s embedding, then we multiply that by a weight matrix, and apply softmax, so then we have probability distribution over our classes (sentiment classes) | then we can concatenate every words together, and keep recursing till we finish the whole sentence | the problem with this model, is that we are losing the pair wise interaction between the two words, | wat we can do it introduce a tensor V, that would capture this interaction ### CNN for Text Classification | we want to use CNNs with text, so we would have some filters | the idea is to treat sentences as one dimensional vector, and then we can apply windows that contain bunch of words to some filters, and aggregate them | . Doc2Vec . as we have representation of words, we can also have the same for sentences, or documents ### Bag of words | for each sentence, count the frequency of each word in your vocab #### weakness | lose ordering or words | lose semantics or words &quot;powerful&quot; should be closer to &quot;strong&quot; than &quot;Paris&quot; | . | . Paragraph vector . for every paragraph, we would have a vector representing it, then we can average those together, and try to get the target paragraph | we can do it as CBOW, and instead of words, we would have paragraphs | . FastText . the idea is that we take a sentence(a bag) of words, or N-grams and then sum their vectors together, then project them to latent space, and then project them again to output space, and apply non-linearity (softmax for example), then apply cross-entropy as a loss function | we can also normalize our bag of features (the word representation), so we down weight most frequent words | instead of softmax, we can use hierarchial softmax, to decrease the training time | this model is super fast, and gives results similar to non-linear complicated models like CNNs | . Hierarchial Attention Networks for Document Classification . the idea is that we want to do document classification | so in the end we want to represent the document by a vector, that we can enter to softmax, and then output a class | we can use think of document as they are formed of sentences, and sentences are formed of words | so we can use GRU based sequence encoder to represent words and then sentences | . GRU . we have a reset gate, and update gate | the idea is that at every step we have a previous hidden output, and a current input | then we have an update gate that determines the percentage to take from the current hidden output, vs the previous hidden output. | then we also have a reset gate, which determines how much we wanna take from the previous state when calculating the current state | we use the tanh function to calculate the hidden state | we use the sigmoid to calc the parameter Z, which tell us the percentage between the current state, and the previous state output . | we can have a forward, and a backward GRU, and concat them . | then we project these concat words representation, and apply non-linearity | then to calculate the sentence vector out of these word vectors, we apply a weighted average on them. | this works like a simplified version of attention | we can do this weighted average using softmax, but first we need to turn this vector to a scaler, which we can do by applying dot product with &quot;query&quot; or &quot;word context&quot;, like we are doing a query: what is the informative word | this will get us with the alphas, which tell us how much we take from each word vector . | NOTE: cross-entropy with one-hot vector is the same as the log-liklihood . | . Neural Architecture for Named Entity Recognition . LSTM-CRF Model . Normal LSTM . has 3 gates, input gage, forget gate, output gate. #### Conditional Random Field | in NER, the named tags are dependant on each other, for example: B tag, and I tag. | so we want to account for that in our loss | to do that we introduce a new compatibility matrix, to count for this dependency #### Character-based models of words | we need it cause, in production, we might encounter new unseen words, so we make up for that using the character encoding | we want to add character representation with our words representation | so we do a character-based bi-LSTM | and we concatenate the output of the LSTM, with our word representation from the lookup table | . Universal Language Model fine-tuning for Text Classification (ULMFiT) . Language Model: a model that trying to give an understanding for language. Like given few words of the sentence, can we guess the next word. ### Disctimonative fine-tuning | so the idea is to split the model pre-trained parameters for each layer | and to also choose a learning rate for each layer | the early layers would have smaller learning rate, so their weight wouldn&#39;t update as much | . slanted triangular learning rate . the idea is just to increase the LR gradually, till some point, then decrease in again | and we do the increase and decrease linearly, so we end up with the triangular shape ### gradual unfreezing | gradually unfreezing parameters through time | . Natural Machine Translation bu Jointly Learning to Align and Translate . we wanna model the conditional probability $ p(y|x)$. where x is the source sentence, and y is the target sentence | . RNN Encoder-Decoder . Encoder . the encoder gonna encode our entire sentence into a single vector c | we can use LSTM, which will output a sequence of vectors $ h_1, h_2, dots ,h_T$. we can choose c to be just the last vector of the LSTM $h_T$ ### Decoder | we can model $p(y|x)$, as that product of $y_i$ for i from 0 to input time T. | but we can do an approximation, that instead of X, we calc using C which is a representation of X. | and instead of using the previous Y outputs in previous time steps, we can use the previous hidden state | . . BLUE Score ( Bilingual Evaluation Understudy ) . Good reference | provides an automatic score for machine translation | the normal precision gives terrible results | they introduced a modified precision score, which gives score to words up to the maximum number of occurrences in the reference sentences | we need also to account for different grams. | for example, for bi-grams, we would count the bi-grams in the output, and count-clip them at the maximum of the bi-gram in the reference sentences | Pn = BLUE score on n-grams only | Combined Blue score: Bp exp(1/n * sigma(Pn)) Bp: brevity Penalty | it basically penalize short translations | Bp: is one if output is longer than reference | otherwise, it&#39;s exp(1 - (output length / reference length)) ## Sequence to Sequence Learning with Neural Networks | . | One limitation of RNNs is that the output sequence, is the same length as the input sequence | this is using two different LSTMs one for input, and one for output | it stacks multiple LSTMs together creating deep LSTM | reversing the order of the words of the input sentence the intuition is that the first of the output is gonna take most info from the first tokens of the input | . | . Phrase representation . they combined DL approaches like RNNs, with statistical ML approached to enhance the translation | we cen learn word embedding from the translation task | . Attention-based Neural Machine Translation . attention solves the problem of decreasing Blue-score with increasing the sentence length ### Global Attention | with previous approaches, we used a small version of attention, to choose which source vector would have the bigger weight | in this version we do the same, but with different, source-target hidden state vectors | so we attend source and target hidden state vectors | . Local Attention . instead of attending to all of the input space, we can jus attend to a portion of it | it&#39;s faster than global attention | how to choose the portion to attend to, is learnable | . Byte Pair Encoding . the main objective is to reduce the amount of unknown words | the idea is to iterate over the bytes in the sequence and pick the most frequent one and replace it with an unused byte | a byte is a character, or a sequence of characters | we keep doing that till we convert our input sequence to bunch of bytes | at test time, we would have Out Of Vocab words, but we can convert them to known bytes that we extracted in training. | we are trying to find a balance between word-encoding and character-encoding | . Google&#39;s Neural Machine Translation . the first paper to beat statistics MT methods | we will use encoder RNN to encode the input | then we will use attention, to attend to the input | they added 8-layer LSTM with residual connection | they used (Byte-Pair) word-pieces technique | the loss function, is the log likelihood of the output, conditioned on the input, but we wanna to add the GLeu score to penalize depending on the quality of the translation, so we can add the Gleu score to the loss function as a reward, in RL | the did beam-search which penalized small sentences, and added penalty for long sequence contexts | lastly, they quantize the model, and its parameters in inference | . Convolution Sequence to Sequence Learning . we want to use the parallelization of the CNNs to learn seq-to-seq | we add positional embedding to account for the different sentence positions. we didn&#39;t need to do this for RNNs as they process words sequentially by default | The network has and encoder, and a decoder the encoder, process all input | the decoder, only considers the previous inputs | . | we have a stack of conv blocks | for each convolution block, you take a k words, each is d dimensional, then you flatten them to be (kd) dimensional, and apply convolution, which is multiplying by filter of size (2d,kd), then you have an output of (2d) dimension. you talk the first half and dot product it with the sigmoid of the second half. there we applied the non-linearity. | and for each block, we also add residual connection. | lastly, we add attention between our encoder blocks output, and the decoder blocks output | . Attention Is All You Need . in RNNs and CNNs, there&#39;s this inductive bias, that the useful information, is right next to us. while, is Attention we don&#39;t assume that | Just as all attention based models, we need to add positional encoding they do that by fourier expansion | . | all previous work was cross attention between encoder, and decoder | here they introduced, self-attention, where the encoder attend to its inputs | Then the added residual connection and layer normalization | . One Head Attention . we have Query, Key, and Value. we multiply each of them with a weight matrix to add learnable parameters | . | first, we do attention between, the query and the key, they we down-scale the dot product by the square root of the embedding dimension. we choose the square root, because it&#39;s not big nor small number, so we keep the attention weights in a reasonable range | . | then we do a weighted sum between the attention weights and the Value matrix ### Multi Head Attention | then one the most important ideas here is multi-head attention | we make many single head attention, then concatenate them together | . Decoder . the same as the encoder | the main difference, is that we do masked attention, which only attend to previous outputs only | Here, the query is coming from the output sequence, and the key, and value, are coming from the encoder | . Subword Regularization . we want to have my many Subword tokenization for the same word | the multiple subword tokenization works like kind of data augmentation, and also adds a regularization effect | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html",
            "relUrl": "/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "NLP Specialization",
            "content": "Course 1: Classification and vector Spaces . Weak 4 . Hashing . We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space | . Locality senstive hashing . the idea is to put items that are close in the vector space, in the same hashing buckets | we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly . | but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can&#39;t be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words . | . Course 2: Probabilstic Models . Week 1: Autocorrect . to make a simple auto-correction system, you need to perform 4 simple steps: you need to identify the miss spelled words | get the n edit distance correct words | filter these candidates for correct words in the dictionary | change miss spelled word with one that has the highest probability | . | . Week 2: POS using Viterbi algorithm . Part of Speech Tagging (POS) is the process of assigning a part of speech to a word | You can use part of speech tagging for: Identifying named entities | Speech recognition | Co-reference Resolution | . | You can use the probabilities of POS tags happening near one another to come up with the most reasonable output. | . Markov Chains: . You can use Markov chains to identify the probability of the next word. | calculate transmission probability | calculate emission probability | . Viterbi algorithm . calculates a probability for each possible path | a probability for a given state, is the (transition probability * emission probability) | total probability of the path, is to product of all states probabilities | we use a top down dynamic programming algorithm to build the paths matrix | we use sum of logs instead of product of probabilities, to avoid converging to zero values | . Week3 Autocomplete . N-Gram models: . it&#39;s a language model that predicts probabilities of sentences depending on the probabilities of their N-grams | to capture the context of beginning end ending of the sentences, we add start and end tokens to each sentence ### Preplexity: | a measure to calculate how complex a sentence is | humans type low preplex sentences | . out of vocab words and smoothing . we can add UNK token for unseen words in the vocab | we can apply smoothing of interpolation for unseen Ngrams | . Week4: Word vectors using Bag of Words method . we can use self-supervised learning in predicting the next word, to learn the weight matrix . | word2vec . continuous bag-of-words predicts a word in context | . | continuous skip-gram tries to predict the words surrounding input word | . | . | Global Vectors (GloVe) factorizes the corpus word co-occurrence matrix | . | fastText based on skip-gram model | support out-of-vocab words | . | Advanced word embedding methods Deep Learning, contextual embedding | BERT | ELMO | GPT-2 ### Continuous Bag-of-Words Model | . | you choose a center word, and a context words around it, and try to predict the centered word | we model the words by one-hot encoding | then we prepare the input training example feature to be an average of the one-hot vectors of the context words, and the label would be one-hot vector of the center word | after training the model, the word embedding is one of the weight matrices, or an average of them #### Evaluations | Intrinsic evaluation test the relationships between words | Analogies | Clustering | Visualizing | . | Extrinsic evaluation test the embedding on the end task you want to perform (ex. Sentiment Analysis) | evaluates actual usefulness of embeddings | time consuming | more difficult to troubleshoot | . | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/nlp/2022/01/30/Coursera_NLP_Specialization.html",
            "relUrl": "/jupyter/deeplearning/python/nlp/2022/01/30/Coursera_NLP_Specialization.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "CS480/680",
            "content": "Lecture 12 . Gausain process . infinite dimentional gaussian distribution | . Lecture 16 . Convolution NN . a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters | . Residual Networks . even after using Relu, NN can still suffer from gradient vanishing | the idea in to add skip connections so that we can create shorter paths | . Lecture 18 . LSTM vs GRU vs Attention . LSTM: 3 gates, one for the cell state, one for the input, one for the output | GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state . takes less parameters | . | Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token . | . Lecture 20 . Autoencoder . takes different input and generates the same output . used in: . compression | denoising | sparse representation | data generation | . Lecture 21 . Generative models . Variational autoencoders . idea: train the encoder to sample a fixed distribution , | we want the network to sample a fixed distribution that is close to the distribution of the encoder, so that it generate similar outputs to the input, but not the same | . GANS: . - . Lecture 22 . Ensemble Learning: . the idea is to combine the hypothesis of several models to produce a better one #### Bagging choose the class the majority votes #### Weighted majority decrease the weight of corrlelated hypothesis | increase the weight of good hypothesis #### Boosting | . | . | . - idea: when an instance is missclassified by hypothesis, increase its weight so that the next hypothesis is more likely to classify it correctly - can boost a weak learner - makes weighted training set, so that it can focus on missclassified examples - at the end generate weighted hypotheses based on the acc of each hypothsis - Advantages: - no need to learn perfect hypothesis - can boost any weak learning algo - boosting is very simple - has good generalization . Netflex challenge 2006 . Lecture 23 . Normalizing flows . Lecture 24 . Gradient boosting . boosting for regression | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deep%20learning/2021/12/14/CS480-680-Intro-to-Machine-Learning.html",
            "relUrl": "/jupyter/deep%20learning/2021/12/14/CS480-680-Intro-to-Machine-Learning.html",
            "date": " • Dec 14, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Voice Conversion",
            "content": "AutoVC . paper | Demo | Repo | . Speach Split . Paper | Demo | repo | . AutoPST . Paper | Demo | repo | . Sources . IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING | Voice Conversion Challenge 2020 results | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/voiceconversion/2021/11/21/VoiceConversion.html",
            "relUrl": "/jupyter/deeplearning/voiceconversion/2021/11/21/VoiceConversion.html",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Python Notebook",
            "content": "Inheritance . If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn&#39;t overload it | . Ex: . class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don&#39;t override the constructor and use the parent one . Multiple Inheritance . when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority . if the two parent classes have the same function, then Method Resolution Orded (MRO) makes the fist class method to be the one called | . | if we override a function from the parent class in the child class, the cild class has the pariority . | to call the both finctions, then we can call the parent class from the child class directly . ex: in the child class we can do: parent.method() | . | . Underscore . single underscore (Before): used for internal variables | double undescore (After): used also for internal scopes only, and also tells python to change the variable name(mangling) | underscore (After): helps avoid conflicts with key words ex(class, int, etc) . | underscore (Before and Afer): used for thing like __init, main__, etc . | . Decorators . def function(func): def inner(): print(1) func() print(3) return inner @function def print_name(): print(2) print_name() . 1 2 3 . so in this example we decorated a function (print_name), which means we gonna pass the deocared function as a paramemter to the decorator | so with decorator, we call our function in the inner function, the decorate it, then return the decorated function | used when u wanna decorate a multiple function with the same, suppose you have an add and multiply function, and you want to print something at the beginig of them, so you crate a decorator that does that, and decorate the two functions with that. | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/python/2021/11/10/Python.html",
            "relUrl": "/jupyter/python/2021/11/10/Python.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Scribbles Notebook",
            "content": "Transformers . To deal with sequential data we have to options: . 1-D convolution NN processing can be parallel | not practical for long sequences | . | Recurrent NN can&#39;t happen in prallel | have gradient vanshing problem of the squence becomes so long | we have bottleneck at the end of the encoder | . | RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention | Decoder utilzes: context vector | weighted sum of hidden states (h1,h2, ... ) from the encoder | . | . | . Transformers . Encoder . first we do input embedding, and positional embedding | in self attention: we multiply q,w,v by a matrix to do lenear transformation | self attentoion: k q --&gt; scaling down --&gt; softmax --&gt; v | . multi-head attention . works as we use many filters in CNN | in wide attention: it takes every word and spread it multi-head attention | in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN? | . | . Positional info . positional encoding using the rotation sin/cos matrix | positional embedding | . Residual connections . to give the chance to skip some learning parameters if it&#39;s better to minimize the loss | . Layer Normalization . in batch normalization ==&gt; we normalize to zero mean and unity varince | we calculate for all samples in each batch (for each channel ) | . | in layer normalization ==&gt; $y = gamma * x + beta $ where gamm and bata are trainable parametes | calculates for all channles in the same sample | . | in instance normalization ==&gt; calculate for one channel in one sample | . Debugging ML Models . Understand bias-variance diagnoses . getting more data ==&gt; fixes high variance | smaller set of features ==&gt; fixes high variance | . | . Refrence . Prof. Andrew NG Vid | . SVM and Kernels . The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently | . we have $ X(i) in R^{100}$, suppose W can be expressed as a linear combintaion of X . $ W = sum_{i = 1}^{M} alpha_{i} y^i x^i$ (This can be proved with the representer theorem) . vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left. | . optimization problem is : $ min {w,b} {1/2} *||W||^2 $ s.t $y^i*((W^T * x^i) + b) &gt;= 1$ . For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant | . Distributed Training in Pytorch . Pytorch DDP Internal . DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP. The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready. . Backward pass: Because backward() function is called on the loss directly, which out of DDP&#39;s control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization. . DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes. . Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration. . DataParallel VS DistributedDataParallel . DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs. | DataParallel doesn&#39;t support model parallel | . Resources . - https://pytorch.org/docs/master/notes/ddp.html - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training . Complete Statistical Theory of Learning - Vladimir Vapnik . There are two ways for generalization: more data, and complete learning theory | Turing said, that you should imitate intelligent person | . Ref: https://www.youtube.com/watch?v=Ow25mjFjSmg . Statistical Machine Learning . part 1: . deduction vs induction: . deduction: is the process of reasoning from one or more general statements to reach a logically certain conclusion. premises must be correct. | induction: reasoning that construct or evaluates general proposition that are derived from specific examples. we can never be sure our conclusion can be wrong! | machine learning tries to automate the process of inductive inference. #### why should ML work? | ML tries to find patterns in data | we will only be able to learn if there&#39;s something to learn | ML makes some assumptions, which are are rarely made explicit. | we need to have an idea what we are looking for. This is called &quot;inductive bias&quot;. Learning is impossible without such a bias the formal theorem if this is called no free lunch theorem | . | on the other hand, if we have a very strong inductive bias, then with just few training examples, then we can have high certainty in the output | the problem of selecting a good hypothesis class is called model selection. | any system that learns has an inductive bias. | if the algorithm works, THERE HAS TO BE A BIAS | the inductive bias, rules over our function space ### Part 2: | it;s not hard for ML algorithm to correctly predict training labels | usually ML algorithms make training errors, that is the function,they come up with doesn&#39;t perfectly fit the training data | what we care about is the performance on teh test set | it&#39;s not always the case that lowering the train data would lower that test data #### K-Nearest algorithms: | for K-nearest algorithm, the best value for K is log(N) | if k is too small ==&gt; overfitting | if k is too large ==&gt; underfitting | . | k nearest algo achieves good results on MNIST dataset for classifying two classes, with simple euclidean distance function | k-nearest can be used for density estimation, clustering, outlier detection | the inductive bias in K nearest algo, is that near points are the of teh same category | the challenging part about k nearest algo is how to measure the distance between points | . Part3 . for ML, we don&#39;t put any assmuptions for the data probability ditribution | often, the input and output are random variables | in some applications, it&#39;s important that the loss depends on the input X. | also in some cases, the type of error is critical, for exp. spaam detection | Bayes risk: is the min error for the expected values over all examples --&gt; basically the it&#39;s the lowest error you can achieve | Consistency: we say algorithm A is consistent, if we have an infinite iid datapoints, and the risk of algorithm&#39;s selected function converges to the Baye&#39;s risk. basically means that if we have infinite data samples, then our algorthms reaches the Bayes risk, which is the lowest error possible | . | Universally consistent: no mattter the underlying probability distribution is, when we have enough data points, the algorithm would be consistent consistent independantly of the data distribution | KNN classifier, SVM, boositn, random forests are universally consistent | . | . DL Book . CH1 . one of the key ideas in DL is that data representation matters a lot, and that DL is a technique for learning how to represent the data | . AI . in AI we need the computer to do some tasks like humans | we do that by providing the computer with a lot of rules describing the world and how to act in different scenarios ### ML | in machine learning we can learn these rules without explicitly told them | but we still need to be provided with custom features that are given by domain experts | . Representation learning . a specific type of ML where we don&#39;t tell the computer the specific features | instead, we give the computer raw input, and it should learn the more complex features explicitly | ex: autoencoders ### DL | is a representation learning algorithms that is applied in multi sequential manner | . . CH2 . Deep Generative Modeling . latent variable: it&#39;s a variable that is controlling some behaviors, but we can&#39;t directly observe it | we are trying to observe true explanatory factors, for example, latent variables, from only observed data | . Autoencoders: . the encoder learns to map from data, to a low-dimensional latent space | the decoder learns to map back from the low-dimensional space back into a reconstructed observation | the bottleneck hidden layer forces the network to learn a compressed latent representation | the reconstruction loss forces the latent representation to capture as much information from the data | . Variational Autoencoders (VAE) . with classic autoencoders, once we train the network, then the latent representation is deterministic | but in VAE, we add some randomness, so we can generate new samples | so the encoder should output a mean and a standard deviation, which represents a distribution of the input, then we can sample from this distribution to generate new sample | the encoder is trying to infer a probability distribution of the latent space with respect to its input data | the decoder is trying to infer a new probability distribution over the input space given the latent space | the loss is going to be function of the parameters of the two distributions | the loss would consist of a construction loss and a regularization term, which is responsible for inducing some notion of structure of this probabilistic space | We need regularization and a prior to: continuity: points that are close in the latent space, should remain close after decoding | completeness: samples from the latent space, should produce meaning content after decoding | . | we can&#39;t perform back propagation, as there&#39;s stochasticity in the latent space, | to solve this issue, we fix, the mean and variance, and introduce the stochastic term separate from them . | The key problem with VAEs, is a concern of density estimation . | . Generative Adversarial Networks (GANs) . we need to sample from a very complex distribution, that we don&#39;t know, and can&#39;t estimate | the solution, is to sample from a simple distribution (eg. noise), then learn a transformation, to the data distribution | we have a Generator, that&#39;s tries to transform the data sampled from the random noise, into data that looks real, to trick the discriminator | we have a discriminator, which tries to identify real data from fake. | . SubWords . it&#39;s just like the skip-gram model | but we just changed the score function | so that we increased the vocab size by adding N-grams of all words we have | then we use them to capture more meaning from the words | if we encounter new word, then we add it&#39;s N-gram and thats would be the word vector ## Decision Trees | they are greedy algorithm | they can stuck in local minimum | if the we have some continuous features, we can use it multiple times, every time with different range ## Purity function | we want to define a purity function, that has the following it has it&#39;s maximum value when probability of any class in 1 | it has it&#39;s minimum value when all classes has the same probability | Purity( pa, pb) == Purity (pb, pa) | . | entropy = impurity = -1 * purity | one function that satisfies all these requirements, is . $ purity(p_{c1}, p_{c2}, p_{c3}) = p_{c1} log(p_{c1}) * p_{c2} log(p_{c2}) * p_{c3} log(p_{c3})$ | . | so we choose features, that would increase purity the most after splitting the dataset using it | to calculate after entropy or purity of a set after seperation, would be the weighted average of the subsets | . Why going deep in Deep Learning . one motivation, is that going deep can reduce the size of our units exponentially | in our underlying function, there could be some symmetry, that we can fold the function across its axis | for statistical reasons, we would want to infer out initial beliefs, about our function, that is involve the composition of several simpler functions | Empirical experiments show that deeper networks generalize better | . Few-shot Learning . we want to classify examples, that we only have few examples for, maybe even 0 | the idea is instead of having a classifier as the last layer(softmax layer), we can use a siamese network, just to tell us is the two examples are similar | so we just learn a similarity function | . GLIDE: . Generates realistic images from text | GLIDE is the next version of DALLE, with respect to photo realism and caption similarity | this is fine-tuned model, not zero-shot like DALLE | It can generate or edit images | so you can generate an image using zero-shot, then you can edit the image by putting masks on the image and tell the model what to draw in the masked area | . Diffusion Models . we start with the original image, then we keep add noise to it till it become so noisy | then we try to reverse the operation and get it back to the input image | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html",
            "relUrl": "/jupyter/deeplearning/python/2021/10/27/scribbles.html",
            "date": " • Oct 27, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "TTS Research",
            "content": "TTS . TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. . End-to-end TTS . easier pipeline | better peformance | . Wavenet . Based on: . DeepMind&#39;s Wavenet | https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html | https://deepmind.com/blog/article/wavenet-generative-model-raw-audio | . Wavenet V1 . before wavenet, ther was two methods: . generative method, which would produce the over all song of the sentece well, but would fail to produce the individual sounds well | concatinative: we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence . | wavenet: tries to do both of the above methods, it also can change the speaker by changing some parameters . | data output: 16 khz rate . | we cant use normal RNN as the max seq length around 50 . | they used dilated CNNs: . can have very long look back | fast to train | . | . This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. . Wavenet V2 . The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn&#39;t depend on previous samples to produce the current sample, while still maintaining the same quality . End-to-end adversarial TTS . Based on: . https://www.youtube.com/watch?v=WTB2p4bqtXU | https://deepmind.com/research/publications/2020/End-to-End-Adversarial-Text-to-Speech | https://arxiv.org/abs/2006.03575 | . Adversarial: means we have a generator and a descriminator which tries to detect which output is generated and which is real End-to-end : they take in text and output the speech . steps: . we enter the text, then we tokenize it | then we use a stack of dilated conv layers to predict the length of each token | with this info, we can predict the center of each token | then we use a gaussian kernel to give a prob distributaion for the place of the token center | then the generator can generate the sound | . ** Q: here we assumed that every token starts directly after the one before it, but is that the case? don&#39;t we need to add a small duration of no sound between tokens? . Tacotron2 . based on: . paper | repo | . you train tacotron-like seq2seq model to output a mel spectrogram, then pass that to wavenet to generate the wave form . This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech. . Common TTS Architictures: . - Autoregressive models - DCCN - Flow - Teacher Student - Variational auto encoders (VAE) - GANS . Resources . https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2 | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/tts/deeplearning/2021/10/14/Text-To-Speech.html",
            "relUrl": "/tts/deeplearning/2021/10/14/Text-To-Speech.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "The Science of Well Being",
            "content": "Misconcepions about happiness . Why our expectations are so bad . How can we overcome our biases . What&#39;s really makes us happy . Putting strategies into practice .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/2021/10/06/The-Science-of-Well-Being.html",
            "relUrl": "/2021/10/06/The-Science-of-Well-Being.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "FastAi 2020",
            "content": "Lecture two . P value: . determines if some numbers have realationship, or they are random (whether they are independat or dependant) suppose we have the temp and R (transmitity) values of a 100 cities in China and we want to see if there&#39;s a relation between them. then we generate many sets of random numbers for each parameter then we calculate the P value which would tell us what&#39;s the percentage this slope is a random, and that ther&#39;s no relation A P-value is the probability of an observed result assuming that the null hypothesis (there&#39;s no relation ) is true PS: P-value also is dependant on the size of the set u used, so they don&#39;t measure the importance of the result. so don&#39;t use P-values If the P value is &gt; 0.5 then we sure that these daata have no ralation, and if the p-value is so small, then there&#39;s a chance that the data have a relation . Lecture three . In the course video and book, we built a bear classifier, using data from Microsoft Ping Api. . To build a deep learning model, we have first to gather the data, then we should prepare the data to be in the right format for the model, then we train the model and observe if we get satisfiable results, if not then we try to investigate to try to get better results. Finally we save our model and deploy it! . while gathering the data, notice that all the time the data would be biased, and in sometimes these biases would be severe that they can&#39;t be ignored . Race classifier . I have tried to rebuild the notebook and to make a Race classifier. . I got the dataset from here Dataset, and then trained a small Resnet18 NN to classify images. . To deploy the app, I used Voila and MyBinder to make it available online here: Race Classifier . Lastly, all the code can be found in this githun repo here .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/fastai/2021/10/06/Fastai-2020.html",
            "relUrl": "/jupyter/fastai/2021/10/06/Fastai-2020.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Lecture 1",
            "content": "This is Post for Computational Linear Algebra . import numpy as np . a = np.array( [[6,5,3,1], [3,6,2,2], [3,4,3,1] ]) b = np.array( [ [1.5 ,1], [2,2.5], [5 ,4.5] ,[16 ,17] ]) . for c in (a @ b): print(c) . [50. 49.] [58.5 61. ] [43.5 43.5] . Lecture 2 . Matrix decomposition: we decopose matricies into smaller ones that has special properties . Singular Value Decomposition (SVD): . it&#39;s an exact decomposition, so you can retrieve the orginal matrix again | . Some SVD applications: . semantic analysis | collaborative filtering / recommendation | data compression | PCA (principal component analysis) | . Non-negative Matrix Factorization (NMF) .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/2021/10/02/Computational-Linear-Algebra.html",
            "relUrl": "/2021/10/02/Computational-Linear-Algebra.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . Running cells with &#39;Python 3.9.4 64-bit&#39; requires ipykernel package. Run the following command to install &#39;ipykernel&#39; into the Python environment. Command: &#39;/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall&#39; . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bodasadalla98.github.io/Boda-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bodasadalla98.github.io/Boda-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bodasadalla98.github.io/Boda-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}