{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908c9a47",
   "metadata": {},
   "source": [
    "# Applied Deep Learning\n",
    "> Applied Deep Learning [Course](https://github.com/maziarraissi/Applied-Deep-Learning) \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae636b1",
   "metadata": {},
   "source": [
    "## Deep Learning overview\n",
    "* we can look at deep learning as an algorithm that writes algorithms, like a compiler\n",
    " - in this case the source code would be the data: (examples/experiences)\n",
    " - excutable code would be the deployable model \n",
    "\n",
    " * Deep: Functions compositions  $ f_l f_{l-1} .... f_1$\n",
    " * Learning: Loss, Back-propagation, and Gradient Descent\n",
    "\n",
    " * $ L(\\theta) \\approx J(\\theta)$ --> noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent \n",
    " * why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow \n",
    " ### Optimizers\n",
    " * to make gradient descent faster, we can add momentum to it.\n",
    "  * another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fe29",
   "metadata": {},
   "source": [
    "  * RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. \n",
    "\n",
    "  ![](assets/Applied_deep_learning/rprop.png)\n",
    "\n",
    "\n",
    "  * Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e06d5f",
   "metadata": {},
   "source": [
    "* Adam:\n",
    "    - can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)\n",
    "    - can also has momentum for all parameter wich can lead to faster convergence\n",
    "* Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53571c3",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "* A simple method to prevent the NN from overfitting \n",
    "* CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image \n",
    "* you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82030f",
   "metadata": {},
   "source": [
    "### Network In Network\n",
    "* the main idea is to put a network inside another network\n",
    "\n",
    "* they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers\n",
    "* this idea is bisacally a (one to one convution) \n",
    "* they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0739a3",
   "metadata": {},
   "source": [
    "* one by one convolution is a normal convolution with fliter size of 1 by 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b1273",
   "metadata": {},
   "source": [
    "* in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had  slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant)\n",
    "* we can achieve local invariant with pooling, and deal with global invariant with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5aec",
   "metadata": {},
   "source": [
    "\n",
    "### VGG Net\n",
    "\n",
    "#### Local Response Normalization:\n",
    "* the idea is to normalize a pixel across nearing channels \n",
    "\n",
    "* after comparing nets with lrn and nets without, they didn't find big difference, so they stoped using it \n",
    "\n",
    "#### Data Augmentation\n",
    "* Image translations( random crops), and horizontal reflection \n",
    "* altering the intensities of the RGB channels \n",
    "* scale jittering   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c030d81",
   "metadata": {},
   "source": [
    "### GoogleNet\n",
    "\n",
    "* You stack multiple inception modules on top of each ohter \n",
    "* the idea is that you don't have to choose which filter size to use, so why don't use them all \n",
    "* to make the network more efficient, they first projected the input with one by one convolution then applied the main filters \n",
    "* you concatinate the many filters through the channel dimension    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab15387",
   "metadata": {},
   "source": [
    "### Batch Normalization \n",
    "\n",
    "* The main goal of batch normalization is to redude the `Internal Covariant Shift`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f025de2",
   "metadata": {},
   "source": [
    "* we can just normalize the inputs and it would work fine\n",
    "* the problem is that in each following layer, and statistics of its output would depend on its weights \n",
    "* so we also need to nomalize the inputs in hidden layers \n",
    "* here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen\n",
    "\n",
    "* in inference we can't have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset \n",
    "\n",
    "#### conv layers\n",
    "* for conv layers we apply normalization across every channel for every pixel in the batch of images\n",
    "* the effective bach size would be ==> m*p*q where m is the number of images in the batch and \n",
    "p,q are the image resolution \n",
    "\n",
    "#### Benifits of batch norm:\n",
    "* you can use higher learning rate, as the training is more stable \n",
    "* less sensitive to initialization \n",
    "* less sensitive to activation function \n",
    "* it has regularization effects, because thre's random mini batch every time \n",
    "* preserve gradient magintude ?? maybe --> because the jacobian doesn't scale as we scales the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a127c4",
   "metadata": {},
   "source": [
    "### Parametric Relu:\n",
    "\n",
    "$ f({y_i}) = \\max(0,y_i) + a_i \\min(0, y_i) $\n",
    "* if $a_i = 0$  --> Relu\n",
    "* if $a_i = 0.01$ --> Leaky Relu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceafe0",
   "metadata": {},
   "source": [
    "* the initialization of weights and biases depends on the type of activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiming Initialization  (I didn't fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations):\n",
    "\n",
    "- professor went into deep mathematical details into how to choose the intial values for weights\n",
    "* the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we  want the weights to have values centred around 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127625d2",
   "metadata": {},
   "source": [
    "### Label smoothing regularization\n",
    "- the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f5b57",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
