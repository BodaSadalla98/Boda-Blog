<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Applied Deep Learning | Boda Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Applied Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Applied Deep Learning Course" />
<meta property="og:description" content="Applied Deep Learning Course" />
<link rel="canonical" href="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" />
<meta property="og:url" content="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" />
<meta property="og:site_name" content="Boda Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2022-02-01T00:00:00-06:00","url":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html","@type":"BlogPosting","headline":"Applied Deep Learning","dateModified":"2022-02-01T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html"},"description":"Applied Deep Learning Course","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Boda-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bodasadalla98.github.io/Boda-Blog/feed.xml" title="Boda Blog" /><link rel="shortcut icon" type="image/x-icon" href="/Boda-Blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Boda-Blog/">Boda Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Boda-Blog/about/">About Me</a><a class="page-link" href="/Boda-Blog/search/">Search</a><a class="page-link" href="/Boda-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Applied Deep Learning</h1><p class="page-description">Applied Deep Learning <a href='https://github.com/maziarraissi/Applied-Deep-Learning'>Course</a></p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      31 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#python">python</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/BodaSadalla98/Boda-Blog/tree/master/_notebooks/2022-02-01-Applied_Deep_Learning.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Boda-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/BodaSadalla98/Boda-Blog/master?filepath=_notebooks%2F2022-02-01-Applied_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/BodaSadalla98/Boda-Blog/blob/master/_notebooks/2022-02-01-Applied_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Reference">Reference </a></li>
<li class="toc-entry toc-h2"><a href="#Deep-Learning-overview">Deep Learning overview </a></li>
<li class="toc-entry toc-h2"><a href="#Dropout">Dropout </a></li>
<li class="toc-entry toc-h1"><a href="#Computer-Vision">Computer Vision </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Image-Classification">Image Classification </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Large-Networks">Large Networks </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Network-In-Network">Network In Network </a></li>
<li class="toc-entry toc-h4"><a href="#VGG-Net">VGG Net </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Local-Response-Normalization:">Local Response Normalization: </a></li>
<li class="toc-entry toc-h5"><a href="#Data-Augmentation">Data Augmentation </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#GoogleNet">GoogleNet </a></li>
<li class="toc-entry toc-h4"><a href="#Batch-Normalization">Batch Normalization </a>
<ul>
<li class="toc-entry toc-h5"><a href="#conv-layers">conv layers </a></li>
<li class="toc-entry toc-h5"><a href="#Benifits-of-batch-norm:">Benifits of batch norm: </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#Parametric-Relu:">Parametric Relu: </a></li>
<li class="toc-entry toc-h4"><a href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):">Kaiming Initialization  (I didn&#39;t fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations): </a></li>
<li class="toc-entry toc-h4"><a href="#Label-smoothing-regularization">Label smoothing regularization </a></li>
<li class="toc-entry toc-h4"><a href="#ResNet">ResNet </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Identity-mapping-in-resnets">Identity mapping in resnets </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#Wide-Residual-Networks">Wide Residual Networks </a></li>
<li class="toc-entry toc-h4"><a href="#ResNext">ResNext </a></li>
<li class="toc-entry toc-h4"><a href="#Squeeze-and-Ecxcitation-Networks">Squeeze-and-Ecxcitation Networks </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Squeeze-:-just-a-global-averaging-step">Squeeze : just a global averaging step </a></li>
<li class="toc-entry toc-h5"><a href="#Excitation:-is-just-a-fully-connected-newtwork">Excitation: is just a fully connected newtwork </a></li>
<li class="toc-entry toc-h5"><a href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention">Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#Spatial-Transformer-Network">Spatial Transformer Network </a></li>
<li class="toc-entry toc-h4"><a href="#Dynamic-Routing-between-capsuls">Dynamic Routing between capsuls </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Small-Networks">Small Networks </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Knowledge-Distillation">Knowledge Distillation </a></li>
<li class="toc-entry toc-h4"><a href="#Network-Pruning:">Network Pruning: </a></li>
<li class="toc-entry toc-h4"><a href="#Quantization">Quantization </a></li>
<li class="toc-entry toc-h4"><a href="#Huffman-Coding">Huffman Coding </a></li>
<li class="toc-entry toc-h4"><a href="#Squeeze-Net">Squeeze Net </a></li>
<li class="toc-entry toc-h4"><a href="#XNOR-NET">XNOR-NET </a></li>
<li class="toc-entry toc-h4"><a href="#Mobile-Nets">Mobile Nets </a></li>
<li class="toc-entry toc-h4"><a href="#Xception">Xception </a></li>
<li class="toc-entry toc-h4"><a href="#Mobile-Net-V2">Mobile Net V2 </a></li>
<li class="toc-entry toc-h4"><a href="#ShuffleNet">ShuffleNet </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Auto-ML">Auto ML </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Regularized-Evolution">Regularized Evolution </a></li>
<li class="toc-entry toc-h4"><a href="#EfficientNet">EfficientNet </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Robustness">Robustness </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Intrigiong-peroperties-of-neural-networks">Intrigiong peroperties of neural networks </a></li>
<li class="toc-entry toc-h4"><a href="#Towards-Evaluating-the-Robustness-of-Neural-Networks">Towards Evaluating the Robustness of Neural Networks </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Visualizing-&-Understanding">Visualizing &amp; Understanding </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Understanding-Deep-Learning-Requires-Rethinking-Generalization">Understanding Deep Learning Requires Rethinking Generalization </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Transfer-Learning">Transfer Learning </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Image-Transformation">Image Transformation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Semantic-Segmentation">Semantic Segmentation </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Atrous-Convolution:">Atrous Convolution: </a></li>
<li class="toc-entry toc-h4"><a href="#Dilated-Convolution:">Dilated Convolution: </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Image-Super-Resolution">Image Super-Resolution </a></li>
<li class="toc-entry toc-h3"><a href="#Perceptual-Losses">Perceptual Losses </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Object-Detection">Object Detection </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Two-Stage-Detectors">Two Stage Detectors </a>
<ul>
<li class="toc-entry toc-h4"><a href="#R-CNN">R-CNN </a></li>
<li class="toc-entry toc-h4"><a href="#Spatial-Pyramid-Pooling">Spatial Pyramid Pooling </a></li>
<li class="toc-entry toc-h4"><a href="#Fast-R-CNN">Fast R-CNN </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#One-Stage-Detectors">One Stage Detectors </a>
<ul>
<li class="toc-entry toc-h4"><a href="#YOLO">YOLO </a></li>
<li class="toc-entry toc-h4"><a href="#SSD:-Single-Shot-MultiBox-Detector">SSD: Single Shot MultiBox Detector </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Video">Video </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Large-scale-video-classification">Large-scale video classification </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Early-Fusion">Early Fusion </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Two-Stream-CNN-for-action-recognition">Two-Stream CNN for action recognition </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Optical-flow-stacking">Optical flow stacking </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Non-local-Neural-Network">Non-local Neural Network </a></li>
<li class="toc-entry toc-h3"><a href="#Group-Normalization">Group Normalization </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Natural-Language-Processing">Natural Language Processing </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Word-Representation">Word Representation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Distributed-Representation-of-Words-and-Phrases-and-their-Compositionality">Distributed Representation of Words and Phrases and their Compositionality </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Word2Vec-(-Efficient-Estimation-of-Word-Representation-in-Vector-Space-)">Word2Vec ( Efficient Estimation of Word Representation in Vector Space ) </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Skip-gram-Model">Skip-gram Model </a>
<ul>
<li class="toc-entry toc-h6"><a href="#Evaluation">Evaluation </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#GloVe:-Global-Vectors-for-Word-Representation">GloVe: Global Vectors for Word Representation </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Text-Classification">Text Classification </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Recursive-deep-models-for-semantic-compositionality-over-sentiment-Treebank">Recursive deep models for semantic compositionality over sentiment Treebank </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Doc2Vec">Doc2Vec </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Paragraph-vector">Paragraph vector </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#FastText">FastText </a></li>
<li class="toc-entry toc-h2"><a href="#Hierarchial-Attention-Networks-for-Document-Classification">Hierarchial Attention Networks for Document Classification </a>
<ul>
<li class="toc-entry toc-h3"><a href="#GRU">GRU </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Neural-Architecture-for-Named-Entity-Recognition">Neural Architecture for Named Entity Recognition </a>
<ul>
<li class="toc-entry toc-h3"><a href="#LSTM-CRF-Model">LSTM-CRF Model </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Normal-LSTM">Normal LSTM </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Universal-Language-Model-fine-tuning-for-Text-Classification-(ULMFiT)">Universal Language Model fine-tuning for Text Classification (ULMFiT) </a>
<ul>
<li class="toc-entry toc-h3"><a href="#slanted-triangular-learning-rate">slanted triangular learning rate </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Natural-Machine-Translation-bu-Jointly-Learning-to-Align-and-Translate">Natural Machine Translation bu Jointly Learning to Align and Translate </a>
<ul>
<li class="toc-entry toc-h3"><a href="#RNN-Encoder-Decoder">RNN Encoder-Decoder </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Encoder">Encoder </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#BLUE-Score-(-Bilingual-Evaluation-Understudy-)">BLUE Score ( Bilingual Evaluation Understudy ) </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Phrase-representation">Phrase representation </a></li>
<li class="toc-entry toc-h2"><a href="#Attention-based-Neural-Machine-Translation">Attention-based Neural Machine Translation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Local-Attention">Local Attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Byte-Pair-Encoding">Byte Pair Encoding </a></li>
<li class="toc-entry toc-h2"><a href="#Google's-Neural-Machine-Translation">Google&#39;s Neural Machine Translation </a></li>
<li class="toc-entry toc-h2"><a href="#Convolution-Sequence-to-Sequence-Learning">Convolution Sequence to Sequence Learning </a></li>
<li class="toc-entry toc-h2"><a href="#Attention-Is-All-You-Need">Attention Is All You Need </a>
<ul>
<li class="toc-entry toc-h3"><a href="#One-Head-Attention">One Head Attention </a></li>
<li class="toc-entry toc-h3"><a href="#Decoder">Decoder </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-01-Applied_Deep_Learning.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reference">
<a class="anchor" href="#Reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference<a class="anchor-link" href="#Reference"> </a>
</h2>
<p><a href="https://github.com/maziarraissi/Applied-Deep-Learning">Repo with the slides, and course info</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-Learning-overview">
<a class="anchor" href="#Deep-Learning-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning overview<a class="anchor-link" href="#Deep-Learning-overview"> </a>
</h2>
<ul>
<li>
<p>we can look at deep learning as an algorithm that writes algorithms, like a compiler</p>
<ul>
<li>in this case the source code would be the data: (examples/experiences)</li>
<li>
<p>excutable code would be the deployable model</p>
</li>
<li>
<p>Deep: Functions compositions  $ f_l f_{l-1} .... f_1$</p>
</li>
<li>
<p>Learning: Loss, Back-propagation, and Gradient Descent</p>
</li>
<li>
<p>$ L(\theta) \approx J(\theta)$ --&gt; noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent</p>
</li>
<li>why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow 
### Optimizers</li>
<li>to make gradient descent faster, we can add momentum to it.</li>
<li>another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p>RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign.</p>
<p><img src="assets/Applied_deep_learning/rprop.png" alt=""></p>
</li>
</ul>
<ul>
<li>Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Adam:<ul>
<li>can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)</li>
<li>can also has momentum for all parameter wich can lead to faster convergence</li>
</ul>
</li>
<li>Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dropout">
<a class="anchor" href="#Dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout<a class="anchor-link" href="#Dropout"> </a>
</h2>
<ul>
<li>A simple method to prevent the NN from overfitting </li>
<li>CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image </li>
<li>you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Computer-Vision">
<a class="anchor" href="#Computer-Vision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Computer Vision<a class="anchor-link" href="#Computer-Vision"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Image-Classification">
<a class="anchor" href="#Image-Classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Classification<a class="anchor-link" href="#Image-Classification"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Large-Networks">
<a class="anchor" href="#Large-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Large Networks<a class="anchor-link" href="#Large-Networks"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Network-In-Network">
<a class="anchor" href="#Network-In-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network In Network<a class="anchor-link" href="#Network-In-Network"> </a>
</h4>
<ul>
<li>
<p>the main idea is to put a network inside another network</p>
</li>
<li>
<p>they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers</p>
</li>
<li>this idea is bisacally a (one to one convution) </li>
<li>they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>one by one convolution is a normal convolution with fliter size of 1 by 1 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had  slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant)</li>
<li>we can achieve local invariant with pooling, and deal with global invariant with data augmentation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="VGG-Net">
<a class="anchor" href="#VGG-Net" aria-hidden="true"><span class="octicon octicon-link"></span></a>VGG Net<a class="anchor-link" href="#VGG-Net"> </a>
</h4>
<h5 id="Local-Response-Normalization:">
<a class="anchor" href="#Local-Response-Normalization:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Response Normalization:<a class="anchor-link" href="#Local-Response-Normalization:"> </a>
</h5>
<ul>
<li>
<p>the idea is to normalize a pixel across nearing channels</p>
</li>
<li>
<p>after comparing nets with lrn and nets without, they didn't find big difference, so they stoped using it</p>
</li>
</ul>
<h5 id="Data-Augmentation">
<a class="anchor" href="#Data-Augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Augmentation<a class="anchor-link" href="#Data-Augmentation"> </a>
</h5>
<ul>
<li>Image translations( random crops), and horizontal reflection </li>
<li>altering the intensities of the RGB channels </li>
<li>scale jittering   </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="GoogleNet">
<a class="anchor" href="#GoogleNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>GoogleNet<a class="anchor-link" href="#GoogleNet"> </a>
</h4>
<ul>
<li>You stack multiple inception modules on top of each ohter </li>
<li>the idea is that you don't have to choose which filter size to use, so why don't use them all </li>
<li>to make the network more efficient, they first projected the input with one by one convolution then applied the main filters </li>
<li>you concatinate the many filters through the channel dimension    </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Batch-Normalization">
<a class="anchor" href="#Batch-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization<a class="anchor-link" href="#Batch-Normalization"> </a>
</h4>
<ul>
<li>The main goal of batch normalization is to redude the <code>Internal Covariant Shift</code>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>we can just normalize the inputs and it would work fine</li>
<li>the problem is that in each following layer, and statistics of its output would depend on its weights </li>
<li>so we also need to nomalize the inputs in hidden layers </li>
<li>
<p>here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen</p>
</li>
<li>
<p>in inference we can't have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset</p>
</li>
</ul>
<h5 id="conv-layers">
<a class="anchor" href="#conv-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>conv layers<a class="anchor-link" href="#conv-layers"> </a>
</h5>
<ul>
<li>for conv layers we apply normalization across every channel for every pixel in the batch of images</li>
<li>the effective bach size would be ==&gt; m<em>p</em>q where m is the number of images in the batch and 
p,q are the image resolution </li>
</ul>
<h5 id="Benifits-of-batch-norm:">
<a class="anchor" href="#Benifits-of-batch-norm:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Benifits of batch norm:<a class="anchor-link" href="#Benifits-of-batch-norm:"> </a>
</h5>
<ul>
<li>you can use higher learning rate, as the training is more stable </li>
<li>less sensitive to initialization </li>
<li>less sensitive to activation function </li>
<li>it has regularization effects, because thre's random mini batch every time </li>
<li>preserve gradient magintude ?? maybe --&gt; because the jacobian doesn't scale as we scales the weights </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Parametric-Relu:">
<a class="anchor" href="#Parametric-Relu:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parametric Relu:<a class="anchor-link" href="#Parametric-Relu:"> </a>
</h4>
<p>$ f({y_i}) = \max(0,y_i) + a_i \min(0, y_i) $</p>
<ul>
<li>if $a_i = 0$  --&gt; Relu</li>
<li>if $a_i = 0.01$ --&gt; Leaky Relu</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>the initialization of weights and biases depends on the type of activation function </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):">
<a class="anchor" href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kaiming Initialization  (I didn't fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations):<a class="anchor-link" href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):"> </a>
</h4>
<ul>
<li>professor went into deep mathematical details into how to choose the intial values for weights</li>
<li>the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we  want the weights to have values centred around 1 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Label-smoothing-regularization">
<a class="anchor" href="#Label-smoothing-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Label smoothing regularization<a class="anchor-link" href="#Label-smoothing-regularization"> </a>
</h4>
<ul>
<li>the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="ResNet">
<a class="anchor" href="#ResNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNet<a class="anchor-link" href="#ResNet"> </a>
</h4>
<ul>
<li>The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection.</li>
</ul>
<h5 id="Identity-mapping-in-resnets">
<a class="anchor" href="#Identity-mapping-in-resnets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identity mapping in resnets<a class="anchor-link" href="#Identity-mapping-in-resnets"> </a>
</h5>
<ul>
<li>the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Wide-Residual-Networks">
<a class="anchor" href="#Wide-Residual-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wide Residual Networks<a class="anchor-link" href="#Wide-Residual-Networks"> </a>
</h4>
<ul>
<li>an attempt to make resnets wider and study if that would make them better </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="ResNext">
<a class="anchor" href="#ResNext" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNext<a class="anchor-link" href="#ResNext"> </a>
</h4>
<ul>
<li>just like resnets but they changed bottleneck blocks with group convolution block </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Squeeze-and-Ecxcitation-Networks">
<a class="anchor" href="#Squeeze-and-Ecxcitation-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze-and-Ecxcitation Networks<a class="anchor-link" href="#Squeeze-and-Ecxcitation-Networks"> </a>
</h4>
<h5 id="Squeeze-:-just-a-global-averaging-step">
<a class="anchor" href="#Squeeze-:-just-a-global-averaging-step" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze : just a global averaging step<a class="anchor-link" href="#Squeeze-:-just-a-global-averaging-step"> </a>
</h5>
<h5 id="Excitation:-is-just-a-fully-connected-newtwork">
<a class="anchor" href="#Excitation:-is-just-a-fully-connected-newtwork" aria-hidden="true"><span class="octicon octicon-link"></span></a>Excitation: is just a fully connected newtwork<a class="anchor-link" href="#Excitation:-is-just-a-fully-connected-newtwork"> </a>
</h5>
<h5 id="Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention">
<a class="anchor" href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention<a class="anchor-link" href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention"> </a>
</h5>
<ul>
<li>scaling is you paying different attention to different channels like attention models </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Spatial-Transformer-Network">
<a class="anchor" href="#Spatial-Transformer-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spatial Transformer Network<a class="anchor-link" href="#Spatial-Transformer-Network"> </a>
</h4>
<ul>
<li>
<p>the main idea is to seperate the main object in the image, like putting a box around it and then this box can be resized, shifted, rotated. so in the end we have a focused image that has only the object, and so we can apply convolution on it and it would be easy then</p>
</li>
<li>
<p>the idea is to first find a good transformation parameters theta, you can do that using NN</p>
</li>
<li>then for every position in the output image, you do a bilinear sampling from the input image</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Dynamic-Routing-between-capsuls">
<a class="anchor" href="#Dynamic-Routing-between-capsuls" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dynamic Routing between capsuls<a class="anchor-link" href="#Dynamic-Routing-between-capsuls"> </a>
</h4>
<ul>
<li>the idea is to make the outputs of the capsule has a norm that is the probability that an object is presenet</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Small-Networks">
<a class="anchor" href="#Small-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Small Networks<a class="anchor-link" href="#Small-Networks"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Knowledge-Distillation">
<a class="anchor" href="#Knowledge-Distillation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Knowledge Distillation<a class="anchor-link" href="#Knowledge-Distillation"> </a>
</h4>
<ul>
<li>the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter <code>T</code> that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Network-Pruning:">
<a class="anchor" href="#Network-Pruning:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network Pruning:<a class="anchor-link" href="#Network-Pruning:"> </a>
</h4>
<ul>
<li>all connections with weights below a threshold are removed from the network </li>
<li>weight are sparse now </li>
<li>then we can represent them using fewer bits</li>
</ul>
<h4 id="Quantization">
<a class="anchor" href="#Quantization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quantization<a class="anchor-link" href="#Quantization"> </a>
</h4>
<ul>
<li>we basically cluster our weight to some centroids</li>
<li>the number of centroids for conv layers are more than the ones for FC layers why:<ul>
<li>because conv layer filters are already sparse, we need higher level of accuracy in them</li>
<li>FC layers are so dense that we can tolerate fewer quantization levels </li>
</ul>
</li>
</ul>
<h4 id="Huffman-Coding">
<a class="anchor" href="#Huffman-Coding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Huffman Coding<a class="anchor-link" href="#Huffman-Coding"> </a>
</h4>
<ul>
<li>store the more common symbols with more bits </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Squeeze-Net">
<a class="anchor" href="#Squeeze-Net" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze Net<a class="anchor-link" href="#Squeeze-Net"> </a>
</h4>
<ul>
<li>the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made </li>
<li>the main idea  is to use one by one comvultion to reduce the dimensionality</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="XNOR-NET">
<a class="anchor" href="#XNOR-NET" aria-hidden="true"><span class="octicon octicon-link"></span></a>XNOR-NET<a class="anchor-link" href="#XNOR-NET"> </a>
</h4>
<ul>
<li>the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation</li>
<li>the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==&gt; $W = \alpha * B $ where alpha  is postative 32 bit constant and B is a binary matrix </li>
<li>
<p>then mean we try to train by using a means square error loss function of the original weights and alpha and B</p>
</li>
<li>
<p>I still can't fully understand  how to binarize the input</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Mobile-Nets">
<a class="anchor" href="#Mobile-Nets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mobile Nets<a class="anchor-link" href="#Mobile-Nets"> </a>
</h4>
<ul>
<li>the idea is to reduce computation complexity by doing c   onv for each channel separately, and not across channels.</li>
<li>so we use number of filters as the same as the input channels</li>
<li>but then we will end up with  output size as the input size, so we still need to do one by one convolution to output the correct size</li>
</ul>
<h4 id="Xception">
<a class="anchor" href="#Xception" aria-hidden="true"><span class="octicon octicon-link"></span></a>Xception<a class="anchor-link" href="#Xception"> </a>
</h4>
<ul>
<li>unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size</li>
</ul>
<h4 id="Mobile-Net-V2">
<a class="anchor" href="#Mobile-Net-V2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mobile Net V2<a class="anchor-link" href="#Mobile-Net-V2"> </a>
</h4>
<ul>
<li>the same as MobileNet, but with Residuals connections.</li>
</ul>
<h4 id="ShuffleNet">
<a class="anchor" href="#ShuffleNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>ShuffleNet<a class="anchor-link" href="#ShuffleNet"> </a>
</h4>
<ul>
<li>the idea is to shuffle channels after doing a group convolution </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Auto-ML">
<a class="anchor" href="#Auto-ML" aria-hidden="true"><span class="octicon octicon-link"></span></a>Auto ML<a class="anchor-link" href="#Auto-ML"> </a>
</h3>
<ul>
<li>the question is can we automate architicture engineering, as we automated feature engineering in DL?</li>
<li>we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Regularized-Evolution">
<a class="anchor" href="#Regularized-Evolution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regularized Evolution<a class="anchor-link" href="#Regularized-Evolution"> </a>
</h4>
<ul>
<li>it's basically random search + selection</li>
<li>at first you randomly choose some  architecture  train, and eval on it and push it to to the population</li>
<li>then you sample some arch. from the population</li>
<li>then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples </li>
<li>then remove the oldest arch. in the population</li>
<li>you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="EfficientNet">
<a class="anchor" href="#EfficientNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>EfficientNet<a class="anchor-link" href="#EfficientNet"> </a>
</h4>
<ul>
<li>the idea is that we do grid seach on a small network to come with the best depth scaling coefficient <code>d</code>, width scaling coefficient <code>w</code>, and resolution scalling coefficient <code>r</code>, then we try to find scaling parameter $\phi$, that gives the best accuracy while maintaning the <code>flops</code> under the limit</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Robustness">
<a class="anchor" href="#Robustness" aria-hidden="true"><span class="octicon octicon-link"></span></a>Robustness<a class="anchor-link" href="#Robustness"> </a>
</h3>
<ul>
<li>The main goal is to make your network robust against adverarial attacks</li>
</ul>
<h4 id="Intrigiong-peroperties-of-neural-networks">
<a class="anchor" href="#Intrigiong-peroperties-of-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Intrigiong peroperties of neural networks<a class="anchor-link" href="#Intrigiong-peroperties-of-neural-networks"> </a>
</h4>
<ul>
<li>there's nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters </li>
<li>neural networks  has blind spots, this  means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image  </li>
<li>
<p>Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets</p>
</li>
<li>
<p>you can train your network to defend against attacks but that's expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time.</p>
</li>
<li>
<p>small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality
#### untargeted adversiral examples</p>
</li>
<li>fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example </li>
<li>then you can just add a weighted loss, one for the  orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples </li>
</ul>
<h4 id="Towards-Evaluating-the-Robustness-of-Neural-Networks">
<a class="anchor" href="#Towards-Evaluating-the-Robustness-of-Neural-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Towards Evaluating the Robustness of Neural Networks<a class="anchor-link" href="#Towards-Evaluating-the-Robustness-of-Neural-Networks"> </a>
</h4>
<ul>
<li>another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. </li>
</ul>
<p><img src="/Boda-Blog/images/copied_from_nb/assets/Applied_deep_learning/adversiral-attack-algo.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Visualizing-&amp;-Understanding">
<a class="anchor" href="#Visualizing-&amp;-Understanding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing &amp; Understanding<a class="anchor-link" href="#Visualizing-&amp;-Understanding"> </a>
</h3>
<ul>
<li>now we want to debug our network, to understand how it works</li>
<li>so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input.</li>
<li>so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass.</li>
<li>we call these max locations, switches</li>
<li>the main idea is, visualising the feature maps, gonna help you modify the network </li>
<li>you can have two models that have the same output for the same input but which one do you trust more?<ul>
<li>to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy 
#### LIME: Local Interpretable Model-agnsortic Explanations</li>
</ul>
</li>
<li>you want to trust the model, meaning that you wanna make sure the model prioritized the important features</li>
<li>but you can't interpret non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model prioritized </li>
</ul>
<h4 id="Understanding-Deep-Learning-Requires-Rethinking-Generalization">
<a class="anchor" href="#Understanding-Deep-Learning-Requires-Rethinking-Generalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Understanding Deep Learning Requires Rethinking Generalization<a class="anchor-link" href="#Understanding-Deep-Learning-Requires-Rethinking-Generalization"> </a>
</h4>
<ul>
<li>NN are powerful enough to fit random data, but then it will not generalize for test data </li>
<li>so when we introduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection.</li>
<li>
<p>so, this means: The model architecture itself isn't a sufficient regularizer.</p>
</li>
<li>
<p>Explicit regularization: dropout, weight decay, data augmentation</p>
</li>
<li>Implicit regularization: early stopping</li>
<li>there exist a two-layer NN with Relu activation, that can fit any N random example</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Transfer-Learning">
<a class="anchor" href="#Transfer-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfer Learning<a class="anchor-link" href="#Transfer-Learning"> </a>
</h3>
<ul>
<li>labled data is expensive</li>
<li>you split a data set in half,we find that transfer learning for the same task, have higher acc'</li>
<li>transfer learning with fine-tuned weight is better than locking the learned weights</li>
<li>on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task
#### DeCAF</li>
<li>first layers learn low-level features, whereas latter layers learn semantic or high-lebel features</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Image-Transformation">
<a class="anchor" href="#Image-Transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Transformation<a class="anchor-link" href="#Image-Transformation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Semantic-Segmentation">
<a class="anchor" href="#Semantic-Segmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semantic Segmentation<a class="anchor-link" href="#Semantic-Segmentation"> </a>
</h3>
<ul>
<li>you want to segments different classes in the image</li>
<li>The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions.</li>
</ul>
<h4 id="Atrous-Convolution:">
<a class="anchor" href="#Atrous-Convolution:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Atrous Convolution:<a class="anchor-link" href="#Atrous-Convolution:"> </a>
</h4>
<ul>
<li>you don't wanna lose much info when you do conv,  and then upsample again, so you fill your filter with holes, so that you lose less info</li>
<li>reduce the degree of signal downsampling 
#### CRF: </li>
<li>deals with the reduced localization accuracy due to the Deep Convolution NN invariance </li>
</ul>
<h4 id="Dilated-Convolution:">
<a class="anchor" href="#Dilated-Convolution:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dilated Convolution:<a class="anchor-link" href="#Dilated-Convolution:"> </a>
</h4>
<ul>
<li>basically atrous convolution</li>
<li>increases the  size of the receptive points layer by layer</li>
</ul>
<h3 id="Image-Super-Resolution">
<a class="anchor" href="#Image-Super-Resolution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Super-Resolution<a class="anchor-link" href="#Image-Super-Resolution"> </a>
</h3>
<ul>
<li>we want to develope a NN that can up-sample images</li>
<li>we can do that using convolution </li>
<li>and in the middle we use one to one convolution to work as non-leaner mapping</li>
</ul>
<h3 id="Perceptual-Losses">
<a class="anchor" href="#Perceptual-Losses" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptual Losses<a class="anchor-link" href="#Perceptual-Losses"> </a>
</h3>
<ul>
<li>mse isn't the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same </li>
<li>the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images</li>
<li>so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers
#### Single Image Super-Resolution(SISR)</li>
<li>the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Object-Detection">
<a class="anchor" href="#Object-Detection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Object Detection<a class="anchor-link" href="#Object-Detection"> </a>
</h2>
<h3 id="Two-Stage-Detectors">
<a class="anchor" href="#Two-Stage-Detectors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Two Stage Detectors<a class="anchor-link" href="#Two-Stage-Detectors"> </a>
</h3>
<h4 id="R-CNN">
<a class="anchor" href="#R-CNN" aria-hidden="true"><span class="octicon octicon-link"></span></a>R-CNN<a class="anchor-link" href="#R-CNN"> </a>
</h4>
<ul>
<li>we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called "selective search"</li>
<li>we then enter that to a CNN to do features extractions</li>
<li>at the end we have a per-class classifier </li>
</ul>
<h4 id="Spatial-Pyramid-Pooling">
<a class="anchor" href="#Spatial-Pyramid-Pooling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spatial Pyramid Pooling<a class="anchor-link" href="#Spatial-Pyramid-Pooling"> </a>
</h4>
<ul>
<li>the idea is that we use spatial pyramid pooling to have a fixed length representation for the image</li>
<li>also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image</li>
</ul>
<h4 id="Fast-R-CNN">
<a class="anchor" href="#Fast-R-CNN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fast R-CNN<a class="anchor-link" href="#Fast-R-CNN"> </a>
</h4>
<ul>
<li>just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don't have to calculate many classifiers, one for each class.</li>
<li>also we don't need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map.</li>
<li>last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. 
#### Feature Pyramids</li>
<li>the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes.</li>
<li>to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows</li>
<li>the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image</li>
<li>to overcome this, from each layer we add a connection to the layer below</li>
<li>so we up sample the feature maps first then do one by one convolution to adjust the number of channels,</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="One-Stage-Detectors">
<a class="anchor" href="#One-Stage-Detectors" aria-hidden="true"><span class="octicon octicon-link"></span></a>One Stage Detectors<a class="anchor-link" href="#One-Stage-Detectors"> </a>
</h3>
<h4 id="YOLO">
<a class="anchor" href="#YOLO" aria-hidden="true"><span class="octicon octicon-link"></span></a>YOLO<a class="anchor-link" href="#YOLO"> </a>
</h4>
<ul>
<li>we want the detector to be realtime, so we can detect objects live</li>
<li>divide the input image into  S * S grid</li>
<li>if the center of an object fell inside a cell, that cell is the one responsible to detect that object </li>
<li>each grid cell gonna predict, B bounding boxes, each with confidence score</li>
</ul>
<h4 id="SSD:-Single-Shot-MultiBox-Detector">
<a class="anchor" href="#SSD:-Single-Shot-MultiBox-Detector" aria-hidden="true"><span class="octicon octicon-link"></span></a>SSD: Single Shot MultiBox Detector<a class="anchor-link" href="#SSD:-Single-Shot-MultiBox-Detector"> </a>
</h4>
<ul>
<li>we want to take the speed from YOLO, and the high acc from the two-stage detectors</li>
<li>unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO
#### YOLO9000 - YOLO V2</li>
<li>Tries to improve upon YOLOv1 using idead from fast-CNN and SSD </li>
<li>we can use higher res images in training</li>
<li>can the anchor boxes from the training images not just randomly </li>
<li>introduced passthrough layer</li>
<li>used hierarchical classification to extend many classes </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Video">
<a class="anchor" href="#Video" aria-hidden="true"><span class="octicon octicon-link"></span></a>Video<a class="anchor-link" href="#Video"> </a>
</h2>
<h3 id="Large-scale-video-classification">
<a class="anchor" href="#Large-scale-video-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Large-scale video classification<a class="anchor-link" href="#Large-scale-video-classification"> </a>
</h3>
<ul>
<li>we would have a context stream which learns features on a low res frames.</li>
<li>and a Fovea Stream, which operates on a high res middle portion of the frames </li>
</ul>
<h4 id="Early-Fusion">
<a class="anchor" href="#Early-Fusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Early Fusion<a class="anchor-link" href="#Early-Fusion"> </a>
</h4>
<ul>
<li>the idea is to take a few frames from the middle of the clip, and apply conv on them, the only diff is that we add a new dimension to filters which coreespond to the number of frames </li>
<li>that's just for the first layer, but then it's normal conv
#### Late Fusion</li>
<li>we have two separate single-frame networks, each one takes a diff frame from the clip, and we concatenate them in the end </li>
</ul>
<h3 id="Two-Stream-CNN-for-action-recognition">
<a class="anchor" href="#Two-Stream-CNN-for-action-recognition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Two-Stream CNN for action recognition<a class="anchor-link" href="#Two-Stream-CNN-for-action-recognition"> </a>
</h3>
<ul>
<li>video can be decomposed into spatial and temporal components </li>
</ul>
<h4 id="Optical-flow-stacking">
<a class="anchor" href="#Optical-flow-stacking" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optical flow stacking<a class="anchor-link" href="#Optical-flow-stacking"> </a>
</h4>
<ul>
<li>we can just follow pixels from frame to another, and then create a flow vectors, in the x,y axis </li>
<li>then we can stack these flow vectors 
#### Trajectory stacking</li>
<li>follow the point from frame to another </li>
</ul>
<h3 id="Non-local-Neural-Network">
<a class="anchor" href="#Non-local-Neural-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-local Neural Network<a class="anchor-link" href="#Non-local-Neural-Network"> </a>
</h3>
<ul>
<li>the idea is that we want to see for output pixel, which areas did it pay attention to in the input</li>
<li>so we attention every output with all possible pixels in the input</li>
<li>if we are using it with videos, then we add another dimension for the time</li>
</ul>
<h3 id="Group-Normalization">
<a class="anchor" href="#Group-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Group Normalization<a class="anchor-link" href="#Group-Normalization"> </a>
</h3>
<ul>
<li>Batch norm, is good as long as we have reasonable batch size</li>
<li>but whe we have very small batch size, then batch norm isn't the best </li>
</ul>
<p><strong>Here's diff between normalizaiton methods:</strong></p>
<p><img src="/Boda-Blog/images/copied_from_nb/assets/nomalizations.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Natural-Language-Processing">
<a class="anchor" href="#Natural-Language-Processing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Natural Language Processing<a class="anchor-link" href="#Natural-Language-Processing"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Word-Representation">
<a class="anchor" href="#Word-Representation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word Representation<a class="anchor-link" href="#Word-Representation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Distributed-Representation-of-Words-and-Phrases-and-their-Compositionality">
<a class="anchor" href="#Distributed-Representation-of-Words-and-Phrases-and-their-Compositionality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distributed Representation of Words and Phrases and their Compositionality<a class="anchor-link" href="#Distributed-Representation-of-Words-and-Phrases-and-their-Compositionality"> </a>
</h3>
<h4 id="Word2Vec-(-Efficient-Estimation-of-Word-Representation-in-Vector-Space-)">
<a class="anchor" href="#Word2Vec-(-Efficient-Estimation-of-Word-Representation-in-Vector-Space-)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word2Vec ( Efficient Estimation of Word Representation in Vector Space )<a class="anchor-link" href="#Word2Vec-(-Efficient-Estimation-of-Word-Representation-in-Vector-Space-)"> </a>
</h4>
<ul>
<li>using CBOW model, or skip-gram model</li>
<li>uses the cosine-similarity distance function </li>
</ul>
<h5 id="Skip-gram-Model">
<a class="anchor" href="#Skip-gram-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Skip-gram Model<a class="anchor-link" href="#Skip-gram-Model"> </a>
</h5>
<ul>
<li>
<p>Continuous Bag of Words is the opposite of the skip-gram in the sense of what are we predicting (word vs context)</p>
</li>
<li>
<p>the idea is that you pick a word, and try to predict the context around it</p>
</li>
<li>so you have a word in the middle and try to predict words around in (before, and after), given a defined window size</li>
<li>and our objective is to maximize the liklihood of the context given the reference word </li>
<li>we can use binary trees, to do an approximation, and speed up the softmax caculation, as for every word in the vocab, we would calculate it's softmax with all other words,but now we can use binary trees, and do that in just log(n), using an approximation, that we group words together, and in each level coming from the root, we go right or left, till we reach the word in the leaves </li>
<li>
<p>we can make this even faster, using huffman encoding to assign shorter paths for more frequent words</p>
</li>
<li>
<p>noise sampling: the idea is to give the model negative samples, that doesn't appear together, and give it low probability</p>
</li>
</ul>
<h6 id="Evaluation">
<a class="anchor" href="#Evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation<a class="anchor-link" href="#Evaluation"> </a>
</h6>
<ul>
<li>we can evaluate the model, using syntactic, and semantic analogies </li>
<li>for example, Berlin to Germany is like France to Paris</li>
</ul>
<h4 id="GloVe:-Global-Vectors-for-Word-Representation">
<a class="anchor" href="#GloVe:-Global-Vectors-for-Word-Representation" aria-hidden="true"><span class="octicon octicon-link"></span></a>GloVe: Global Vectors for Word Representation<a class="anchor-link" href="#GloVe:-Global-Vectors-for-Word-Representation"> </a>
</h4>
<ul>
<li>the idea is to use:<ul>
<li>global matrix factorization methods</li>
<li>local context window methods </li>
</ul>
</li>
<li>we compute a co-occurrence method, that holds the counts every two words come after each other, we try to learn two matrieces and two biases, that log(X) = w1 * w2 + b1 + b2 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Text-Classification">
<a class="anchor" href="#Text-Classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Text Classification<a class="anchor-link" href="#Text-Classification"> </a>
</h2>
<h3 id="Recursive-deep-models-for-semantic-compositionality-over-sentiment-Treebank">
<a class="anchor" href="#Recursive-deep-models-for-semantic-compositionality-over-sentiment-Treebank" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recursive deep models for semantic compositionality over sentiment Treebank<a class="anchor-link" href="#Recursive-deep-models-for-semantic-compositionality-over-sentiment-Treebank"> </a>
</h3>
<ul>
<li>the dataset is presented as a tree with leafs as words</li>
<li>the idea is that for evert word we get it's embedding, then we multiply that by a weight matrix, and apply softmax, so then we have probability distribution over our classes (sentiment classes)</li>
<li>then we can concatenate every words together, and keep recursing till we finish the whole sentence </li>
<li>the problem with this model, is that we are losing the pair wise interaction between the two words,</li>
<li>wat we can do it introduce a tensor V, that would capture this interaction<br>
### CNN for Text Classification </li>
<li>we want to use CNNs with text, so we would have some filters</li>
<li>the idea is to treat sentences as one dimensional vector, and then we can apply windows that contain bunch of words to some filters, and aggregate them </li>
</ul>
<h2 id="Doc2Vec">
<a class="anchor" href="#Doc2Vec" aria-hidden="true"><span class="octicon octicon-link"></span></a>Doc2Vec<a class="anchor-link" href="#Doc2Vec"> </a>
</h2>
<ul>
<li>as we have representation of words, we can also have the same for sentences, or documents
### Bag of words</li>
<li>for each sentence, count the frequency of each word in your vocab
#### weakness</li>
<li>lose ordering or words</li>
<li>lose semantics or words <ul>
<li>"powerful" should be closer to "strong" than "Paris" </li>
</ul>
</li>
</ul>
<h3 id="Paragraph-vector">
<a class="anchor" href="#Paragraph-vector" aria-hidden="true"><span class="octicon octicon-link"></span></a>Paragraph vector<a class="anchor-link" href="#Paragraph-vector"> </a>
</h3>
<ul>
<li>for every paragraph, we would have a vector representing it, then we can average those together, and try to get the target paragraph</li>
<li>we can do it as CBOW, and instead of words, we would have paragraphs </li>
</ul>
<h2 id="FastText">
<a class="anchor" href="#FastText" aria-hidden="true"><span class="octicon octicon-link"></span></a>FastText<a class="anchor-link" href="#FastText"> </a>
</h2>
<ul>
<li>the idea is that we take a sentence(a bag) of words, or N-grams and then sum their vectors together, then project them to latent space, and then project them again to output space, and apply non-linearity (softmax for example), then apply cross-entropy as a loss function </li>
<li>we can also normalize our bag of features (the word representation), so we down weight most frequent words </li>
<li>instead of softmax, we can use hierarchial softmax, to decrease the training time</li>
<li>this model is super fast, and gives results similar to non-linear complicated models like CNNs</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hierarchial-Attention-Networks-for-Document-Classification">
<a class="anchor" href="#Hierarchial-Attention-Networks-for-Document-Classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hierarchial Attention Networks for Document Classification<a class="anchor-link" href="#Hierarchial-Attention-Networks-for-Document-Classification"> </a>
</h2>
<ul>
<li>the idea is that we want to do document classification</li>
<li>so in the end we want to represent the document by a vector, that we can enter to softmax, and then output a class </li>
<li>we can use think of document as they are formed of sentences, and sentences are formed of words </li>
<li>so we can use GRU based sequence encoder to represent words and then sentences </li>
</ul>
<h3 id="GRU">
<a class="anchor" href="#GRU" aria-hidden="true"><span class="octicon octicon-link"></span></a>GRU<a class="anchor-link" href="#GRU"> </a>
</h3>
<ul>
<li>we have a reset gate, and update gate</li>
<li>the idea is that at every step we have a previous hidden output, and a current input</li>
<li>then we have an update gate that determines  the percentage to take from the current hidden output, vs the previous hidden output.</li>
<li>then we also have a reset gate, which determines how much we wanna take from the previous state when calculating the current state </li>
<li>we use the tanh function to calculate the hidden state</li>
<li>
<p>we use the sigmoid to calc the parameter Z, which tell us the percentage between the current state, and the previous state output</p>
</li>
<li>
<p>we can have a forward, and a backward GRU, and concat them</p>
</li>
<li>then we project these concat words representation, and apply non-linearity </li>
<li>then to calculate the sentence vector out of these word vectors, we apply a weighted average on them.</li>
<li>this works like a simplified version of attention </li>
<li>we can do this weighted average using softmax, but first we need to turn this vector to a scaler, which we can do by  applying dot product with "query" or "word context", like we are doing a query: what is the informative word </li>
<li>
<p>this will get us with the alphas, which tell us how much we take from each word vector</p>
</li>
<li>
<p>NOTE: cross-entropy with one-hot vector is the same as the log-liklihood</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Neural-Architecture-for-Named-Entity-Recognition">
<a class="anchor" href="#Neural-Architecture-for-Named-Entity-Recognition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural Architecture for Named Entity Recognition<a class="anchor-link" href="#Neural-Architecture-for-Named-Entity-Recognition"> </a>
</h2>
<h3 id="LSTM-CRF-Model">
<a class="anchor" href="#LSTM-CRF-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM-CRF Model<a class="anchor-link" href="#LSTM-CRF-Model"> </a>
</h3>
<h4 id="Normal-LSTM">
<a class="anchor" href="#Normal-LSTM" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normal LSTM<a class="anchor-link" href="#Normal-LSTM"> </a>
</h4>
<ul>
<li>has 3 gates, input gage, forget gate, output gate.
#### Conditional Random Field </li>
<li>in NER, the named tags are dependant on each other, for example: <code>B</code> tag, and <code>I</code> tag.</li>
<li>so we want to account for that in our loss</li>
<li>to do that we introduce a new compatibility matrix, to count for this dependency 
#### Character-based models of words</li>
<li>we need it cause, in production, we might encounter new unseen words, so we make up for that using the character encoding </li>
<li>we want to add character representation with our words representation</li>
<li>so we do a character-based bi-LSTM</li>
<li>and we concatenate the output of the LSTM, with our word representation from the lookup table </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Universal-Language-Model-fine-tuning-for-Text-Classification-(ULMFiT)">
<a class="anchor" href="#Universal-Language-Model-fine-tuning-for-Text-Classification-(ULMFiT)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Universal Language Model fine-tuning for Text Classification (ULMFiT)<a class="anchor-link" href="#Universal-Language-Model-fine-tuning-for-Text-Classification-(ULMFiT)"> </a>
</h2>
<ul>
<li>Language Model: a model that trying to give an understanding for language. Like given few words of the sentence, can we guess the next word. 
### Disctimonative fine-tuning </li>
<li>so the idea is to split the model pre-trained parameters for each layer</li>
<li>and to also choose a learning rate for each layer</li>
<li>the early layers would have smaller learning rate, so their weight wouldn't update as much</li>
</ul>
<h3 id="slanted-triangular-learning-rate">
<a class="anchor" href="#slanted-triangular-learning-rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>slanted triangular learning rate<a class="anchor-link" href="#slanted-triangular-learning-rate"> </a>
</h3>
<ul>
<li>the idea is just to increase the LR gradually, till some point, then decrease in again</li>
<li>and we do the increase and decrease linearly, so we end up with the triangular shape 
### gradual unfreezing </li>
<li>gradually unfreezing parameters through time </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Natural-Machine-Translation-bu-Jointly-Learning-to-Align-and-Translate">
<a class="anchor" href="#Natural-Machine-Translation-bu-Jointly-Learning-to-Align-and-Translate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Natural Machine Translation bu Jointly Learning to Align and Translate<a class="anchor-link" href="#Natural-Machine-Translation-bu-Jointly-Learning-to-Align-and-Translate"> </a>
</h2>
<ul>
<li>we wanna model the conditional probability $ p(y|x)$. where x is the source sentence, and y is the target sentence </li>
</ul>
<h3 id="RNN-Encoder-Decoder">
<a class="anchor" href="#RNN-Encoder-Decoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>RNN Encoder-Decoder<a class="anchor-link" href="#RNN-Encoder-Decoder"> </a>
</h3>
<h4 id="Encoder">
<a class="anchor" href="#Encoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Encoder<a class="anchor-link" href="#Encoder"> </a>
</h4>
<ul>
<li>the encoder gonna encode our entire sentence into a single vector c </li>
<li>we can use LSTM, which will output a sequence of vectors $ h_1, h_2, \dots ,h_T$. we can choose c to be just the last vector of the LSTM $h_T$
### Decoder</li>
<li>we can model $p(y|x)$, as that product of $y_i$ for i from 0 to input time T.</li>
<li>but we can do an approximation, that instead of X, we calc using C which is a representation of X.</li>
<li>and instead of using the previous Y outputs in previous time steps, we can use the previous hidden state</li>
</ul>
<p><img src="/Boda-Blog/images/copied_from_nb/assets/rnn_encoder_decoder.png" alt=""></p>
<h3 id="BLUE-Score-(-Bilingual-Evaluation-Understudy-)">
<a class="anchor" href="#BLUE-Score-(-Bilingual-Evaluation-Understudy-)" aria-hidden="true"><span class="octicon octicon-link"></span></a>BLUE Score ( Bilingual Evaluation Understudy )<a class="anchor-link" href="#BLUE-Score-(-Bilingual-Evaluation-Understudy-)"> </a>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=DejHQYAGb7Q">Good reference</a></li>
<li>provides an automatic score for machine translation </li>
<li>the normal precision gives terrible results</li>
<li>they introduced a modified precision score, which gives score to words up to the maximum number of occurrences in the reference sentences </li>
<li>we need also to account for different grams.</li>
<li>for example, for bi-grams, we would count the bi-grams in the output, and count-clip them at the maximum of the bi-gram in the reference sentences</li>
<li>Pn = BLUE score on n-grams only </li>
<li>Combined Blue score:   Bp exp(1/n * sigma(Pn)) <ul>
<li>Bp: brevity Penalty</li>
<li>it basically penalize short translations </li>
<li>Bp: is one  if output  is longer than reference </li>
<li>otherwise, it's exp(1 -  (output length / reference length))
## Sequence to Sequence Learning with Neural Networks</li>
</ul>
</li>
<li>One limitation of RNNs is that the output sequence, is the same length as the input sequence  </li>
<li>this is using two different LSTMs  one for input, and one for output </li>
<li>it stacks multiple LSTMs together creating deep LSTM</li>
<li>reversing the order of the words of the input sentence <ul>
<li>the intuition is that the first of the output is gonna take most info from the first tokens of the input </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Phrase-representation">
<a class="anchor" href="#Phrase-representation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Phrase representation<a class="anchor-link" href="#Phrase-representation"> </a>
</h2>
<ul>
<li>they combined DL approaches like RNNs, with statistical ML approached to enhance the translation</li>
<li>we cen learn word embedding from the translation task</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-based-Neural-Machine-Translation">
<a class="anchor" href="#Attention-based-Neural-Machine-Translation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention-based Neural Machine Translation<a class="anchor-link" href="#Attention-based-Neural-Machine-Translation"> </a>
</h2>
<ul>
<li>attention solves the problem of decreasing Blue-score with increasing the sentence length 
### Global Attention </li>
<li>with previous approaches, we used a small version of attention, to choose which source vector would have the bigger weight</li>
<li>in this version we do the same, but with different, source-target hidden state vectors</li>
<li>so we attend source and target hidden state vectors <center> <img src="assets/global-attention-model.png">
</center>
</li>
</ul>
<h3 id="Local-Attention">
<a class="anchor" href="#Local-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Attention<a class="anchor-link" href="#Local-Attention"> </a>
</h3>
<ul>
<li>instead of attending to all of the input space, we can jus attend to a portion of it </li>
<li>it's faster than global attention</li>
<li>how to choose the portion to attend to, is learnable <center> <img src="assets/local-attention.png">
</center>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Byte-Pair-Encoding">
<a class="anchor" href="#Byte-Pair-Encoding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Byte Pair Encoding<a class="anchor-link" href="#Byte-Pair-Encoding"> </a>
</h2>
<ul>
<li>the main objective is to reduce the amount of unknown words</li>
<li>the idea is to iterate over the bytes in the sequence and pick the most frequent one and replace it with an unused byte</li>
<li>a byte is a character, or a sequence of characters </li>
<li>we keep doing that till we convert our input sequence to bunch of bytes</li>
<li>at test time, we would have Out Of Vocab words, but we can convert them to known bytes that we extracted in training. </li>
<li>we are trying to find a balance between word-encoding and character-encoding </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Google's-Neural-Machine-Translation">
<a class="anchor" href="#Google's-Neural-Machine-Translation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Google's Neural Machine Translation<a class="anchor-link" href="#Google's-Neural-Machine-Translation"> </a>
</h2>
<ul>
<li>the first paper to beat statistics MT methods </li>
<li>we will use encoder RNN to encode the input</li>
<li>then we will use attention, to attend to the input</li>
<li>they added 8-layer LSTM with residual connection </li>
<li>they used (Byte-Pair) word-pieces technique</li>
<li>the loss function, is the log likelihood of the output, conditioned on the input, but we wanna to add the GLeu score to penalize depending on the quality of the translation, so we can add the Gleu score to the loss function as a reward, in RL</li>
<li>the did beam-search which penalized small sentences, and added penalty for long sequence contexts </li>
<li>lastly, they quantize the model, and its parameters in inference  </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Convolution-Sequence-to-Sequence-Learning">
<a class="anchor" href="#Convolution-Sequence-to-Sequence-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convolution Sequence to Sequence Learning<a class="anchor-link" href="#Convolution-Sequence-to-Sequence-Learning"> </a>
</h2>
<ul>
<li>we want to use the parallelization of the CNNs to learn seq-to-seq </li>
<li>we add positional embedding to account for the different sentence positions. we didn't need to do this for RNNs as they process words sequentially by default </li>
<li>The network has and encoder, and a decoder<ul>
<li>the encoder, process all input </li>
<li>the decoder, only considers the previous inputs</li>
</ul>
</li>
<li>we have a stack of conv blocks</li>
<li>for each convolution block, you take a k words, each is d dimensional, then you flatten them to be (kd) dimensional, and apply convolution, which is multiplying by filter of size (2d,kd), then you have an output of (2d) dimension. you talk the first half and dot product it with the sigmoid of the second half. there we applied the non-linearity.</li>
<li>and for each block, we also add residual connection.</li>
<li>lastly, we add attention between our encoder blocks output, and the decoder blocks output</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-Is-All-You-Need">
<a class="anchor" href="#Attention-Is-All-You-Need" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention Is All You Need<a class="anchor-link" href="#Attention-Is-All-You-Need"> </a>
</h2>
<ul>
<li>in RNNs and CNNs, there's this inductive bias, that the useful information, is right next to us. while, is Attention we don't assume that </li>
<li>Just as all attention based models, we need to add positional encoding<ul>
<li>they do that by fourier expansion </li>
</ul>
</li>
<li>all previous work was cross attention between encoder, and decoder</li>
<li>here they introduced, self-attention, where the encoder attend to its inputs </li>
<li>Then the added residual connection and layer normalization</li>
</ul>
<h3 id="One-Head-Attention">
<a class="anchor" href="#One-Head-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>One Head Attention<a class="anchor-link" href="#One-Head-Attention"> </a>
</h3>
<ul>
<li>we have Query, Key, and Value.<ul>
<li>we multiply each of them with a weight matrix to add learnable parameters</li>
</ul>
</li>
<li>first, we do attention between, the query and the key, they we down-scale the dot product by the square root of the embedding dimension.<ul>
<li>we choose the square root, because it's not big nor small number, so we keep the attention weights in a reasonable range</li>
</ul>
</li>
<li>then we do a weighted sum between the attention weights and the Value matrix 
### Multi Head Attention</li>
<li>then one the most important ideas here is multi-head attention</li>
<li>we make many single head attention, then concatenate them together</li>
</ul>
<h3 id="Decoder">
<a class="anchor" href="#Decoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decoder<a class="anchor-link" href="#Decoder"> </a>
</h3>
<ul>
<li>the same as the encoder</li>
<li>the main difference, is that we do masked attention, which only attend to previous outputs only </li>
<li>Here, the query is coming from the output sequence, and the key, and value, are coming from the encoder </li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="BodaSadalla98/Boda-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Boda-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Boda-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is an unorganized posts in whih I try to summraize my readings as a way to help remember the knowlegde</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/BodaSadalla98" target="_blank" title="BodaSadalla98"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/bodasadala" target="_blank" title="bodasadala"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
