<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Scribbles Notebook | Boda Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Scribbles Notebook" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A bunch of very different topics scribbles" />
<meta property="og:description" content="A bunch of very different topics scribbles" />
<link rel="canonical" href="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" />
<meta property="og:url" content="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" />
<meta property="og:site_name" content="Boda Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-27T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-10-27T00:00:00-05:00","url":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html","@type":"BlogPosting","headline":"Scribbles Notebook","dateModified":"2021-10-27T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html"},"description":"A bunch of very different topics scribbles","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Boda-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bodasadalla98.github.io/Boda-Blog/feed.xml" title="Boda Blog" /><link rel="shortcut icon" type="image/x-icon" href="/Boda-Blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Boda-Blog/">Boda Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Boda-Blog/about/">About Me</a><a class="page-link" href="/Boda-Blog/search/">Search</a><a class="page-link" href="/Boda-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Scribbles Notebook</h1><p class="page-description">A bunch of very different topics scribbles</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-27T00:00:00-05:00" itemprop="datePublished">
        Oct 27, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#python">python</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/BodaSadalla98/Boda-Blog/tree/master/_notebooks/2021-10-27-scribbles.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Boda-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/BodaSadalla98/Boda-Blog/master?filepath=_notebooks%2F2021-10-27-scribbles.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/BodaSadalla98/Boda-Blog/blob/master/_notebooks/2021-10-27-scribbles.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Transformers">Transformers </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Transformers">Transformers </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Encoder">Encoder </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#multi-head--attention">multi-head  attention </a></li>
<li class="toc-entry toc-h3"><a href="#Positional-info">Positional info </a></li>
<li class="toc-entry toc-h3"><a href="#Residual-connections">Residual connections </a></li>
<li class="toc-entry toc-h3"><a href="#Layer-Normalization">Layer Normalization </a></li>
<li class="toc-entry toc-h3"><a href="#Debugging-ML-Models">Debugging ML Models </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Refrence">Refrence </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#SVM-and-Kernels">SVM and Kernels </a></li>
<li class="toc-entry toc-h3"><a href="#Distributed-Training--in-Pytorch">Distributed Training  in Pytorch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Pytorch-DDP-Internal">Pytorch DDP Internal </a></li>
<li class="toc-entry toc-h4"><a href="#DataParallel-VS-DistributedDataParallel">DataParallel VS DistributedDataParallel </a></li>
<li class="toc-entry toc-h4"><a href="#Resources">Resources </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-27-scribbles.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformers">
<a class="anchor" href="#Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers<a class="anchor-link" href="#Transformers"> </a>
</h2>
<p>To deal with sequential data we have to options:</p>
<ul>
<li>1-D convolution NN<ul>
<li>processing can be parallel </li>
<li>not practical for long sequences </li>
</ul>
</li>
<li>Recurrent NN <ul>
<li>can't happen in prallel </li>
<li>have gradient vanshing problem of the squence becomes so long</li>
<li>we have bottleneck at the end of the encoder </li>
</ul>
</li>
<li>RNN with attention mechanism <ul>
<li>to solve the bottleneck problem, we make Encoder-Decoder attention </li>
<li>Decoder utilzes:<ul>
<li>context vector </li>
<li>weighted sum of hidden states (h1,h2, ... ) from the encoder </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Transformers">
<a class="anchor" href="#Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers<a class="anchor-link" href="#Transformers"> </a>
</h3>
<h4 id="Encoder">
<a class="anchor" href="#Encoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Encoder<a class="anchor-link" href="#Encoder"> </a>
</h4>
<ul>
<li>first we do input embedding, and positional embedding </li>
<li>in self attention: we multiply q,w,v by a matrix to do lenear transformation</li>
<li>self attentoion: k <em> q --&gt; scaling down --&gt; softmax --&gt; </em> v </li>
</ul>
<h3 id="multi-head--attention">
<a class="anchor" href="#multi-head--attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>multi-head  attention<a class="anchor-link" href="#multi-head--attention"> </a>
</h3>
<ul>
<li>works as we use many filters in CNN  </li>
<li>in wide attention: it takes every word and spread it multi-head attention</li>
<li>in narrow attention:  it take every word and split it up across the multi-head <ul>
<li>but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?</li>
</ul>
</li>
</ul>
<h3 id="Positional-info">
<a class="anchor" href="#Positional-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>Positional info<a class="anchor-link" href="#Positional-info"> </a>
</h3>
<ul>
<li>positional encoding using the rotation sin/cos matrix </li>
<li>positional embedding </li>
</ul>
<h3 id="Residual-connections">
<a class="anchor" href="#Residual-connections" aria-hidden="true"><span class="octicon octicon-link"></span></a>Residual connections<a class="anchor-link" href="#Residual-connections"> </a>
</h3>
<ul>
<li>to give the chance to skip some learning parameters if it's better to minimize the loss </li>
</ul>
<h3 id="Layer-Normalization">
<a class="anchor" href="#Layer-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layer Normalization<a class="anchor-link" href="#Layer-Normalization"> </a>
</h3>
<ul>
<li>in batch normalization <ul>
<li>==&gt; we normalize to zero mean and unity varince </li>
<li>we calculate for all samples in each batch (for each channel )</li>
</ul>
</li>
<li>in layer normalization <ul>
<li>==&gt;  $y = \gamma * x +  \beta $  where gamm and bata are trainable parametes</li>
<li>calculates  for all  channles in the same sample </li>
</ul>
</li>
<li>in instance normalization ==&gt; calculate for one channel in one sample </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Debugging-ML-Models">
<a class="anchor" href="#Debugging-ML-Models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Debugging ML Models<a class="anchor-link" href="#Debugging-ML-Models"> </a>
</h3>
<ul>
<li>
<p>Understand bias-variance diagnoses</p>
<ul>
<li>getting more data ==&gt; fixes high variance </li>
<li>smaller set of features ==&gt; fixes high variance </li>
</ul>
</li>
</ul>
<h4 id="Refrence">
<a class="anchor" href="#Refrence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refrence<a class="anchor-link" href="#Refrence"> </a>
</h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=ORrStCArmP4">Prof. Andrew NG Vid</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="SVM-and-Kernels">
<a class="anchor" href="#SVM-and-Kernels" aria-hidden="true"><span class="octicon octicon-link"></span></a>SVM and Kernels<a class="anchor-link" href="#SVM-and-Kernels"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Distributed-Training--in-Pytorch">
<a class="anchor" href="#Distributed-Training--in-Pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distributed Training  in Pytorch<a class="anchor-link" href="#Distributed-Training--in-Pytorch"> </a>
</h3>
<h4 id="Pytorch-DDP-Internal">
<a class="anchor" href="#Pytorch-DDP-Internal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pytorch DDP Internal<a class="anchor-link" href="#Pytorch-DDP-Internal"> </a>
</h4>
<p>DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP.
The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state.
DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.</p>
<p>Backward pass: Because <code>backward()</code> function is called on the loss directly, which out of DDP's control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization.</p>
<p>DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes.</p>
<p>Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.</p>
<h4 id="DataParallel-VS-DistributedDataParallel">
<a class="anchor" href="#DataParallel-VS-DistributedDataParallel" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataParallel VS DistributedDataParallel<a class="anchor-link" href="#DataParallel-VS-DistributedDataParallel"> </a>
</h4>
<ul>
<li>
<code>DataParallel</code> is single-process, multi-thread, and only works on a single machine, while <code>DistributedDataParallel</code> is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs.</li>
<li>
<code>DataParallel</code> doesn't support model parallel </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Resources">
<a class="anchor" href="#Resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources<a class="anchor-link" href="#Resources"> </a>
</h4>
<pre><code>- https://pytorch.org/docs/master/notes/ddp.html
- https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training</code></pre>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="BodaSadalla98/Boda-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Boda-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Boda-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is an unorganized posts in whih I try to summraize my readings as a way to help remember the knowlegde</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/BodaSadalla98" target="_blank" title="BodaSadalla98"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/bodasadala" target="_blank" title="bodasadala"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
