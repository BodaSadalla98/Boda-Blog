<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Scribbles Notebook | Boda Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Scribbles Notebook" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A bunch of very different topics scribbles" />
<meta property="og:description" content="A bunch of very different topics scribbles" />
<link rel="canonical" href="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" />
<meta property="og:url" content="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" />
<meta property="og:site_name" content="Boda Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-27T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-10-27T00:00:00-05:00","url":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html","@type":"BlogPosting","headline":"Scribbles Notebook","dateModified":"2021-10-27T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html"},"description":"A bunch of very different topics scribbles","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Boda-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bodasadalla98.github.io/Boda-Blog/feed.xml" title="Boda Blog" /><link rel="shortcut icon" type="image/x-icon" href="/Boda-Blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Boda-Blog/">Boda Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Boda-Blog/about/">About Me</a><a class="page-link" href="/Boda-Blog/search/">Search</a><a class="page-link" href="/Boda-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Scribbles Notebook</h1><p class="page-description">A bunch of very different topics scribbles</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-27T00:00:00-05:00" itemprop="datePublished">
        Oct 27, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#python">python</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/BodaSadalla98/Boda-Blog/tree/master/_notebooks/2021-10-27-scribbles.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Boda-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/BodaSadalla98/Boda-Blog/master?filepath=_notebooks%2F2021-10-27-scribbles.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/BodaSadalla98/Boda-Blog/blob/master/_notebooks/2021-10-27-scribbles.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Transformers">Transformers </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Transformers">Transformers </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Encoder">Encoder </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#multi-head--attention">multi-head  attention </a></li>
<li class="toc-entry toc-h3"><a href="#Positional-info">Positional info </a></li>
<li class="toc-entry toc-h3"><a href="#Residual-connections">Residual connections </a></li>
<li class="toc-entry toc-h3"><a href="#Layer-Normalization">Layer Normalization </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Debugging-ML-Models">Debugging ML Models </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Refrence">Refrence </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#SVM-and-Kernels">SVM and Kernels </a></li>
<li class="toc-entry toc-h2"><a href="#Distributed-Training--in-Pytorch">Distributed Training  in Pytorch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Pytorch-DDP-Internal">Pytorch DDP Internal </a></li>
<li class="toc-entry toc-h3"><a href="#DataParallel-VS-DistributedDataParallel">DataParallel VS DistributedDataParallel </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Resources">Resources </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Complete-Statistical-Theory-of-Learning---Vladimir-Vapnik">Complete Statistical Theory of Learning - Vladimir Vapnik </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Ref:-https://www.youtube.com/watch?v=Ow25mjFjSmg">Ref: https://www.youtube.com/watch?v=Ow25mjFjSmg </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Statistical-Machine-Learning">Statistical Machine Learning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#part-1:">part 1: </a>
<ul>
<li class="toc-entry toc-h4"><a href="#deduction-vs-induction:">deduction vs induction: </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Part3">Part3 </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#DL-Book">DL Book </a>
<ul>
<li class="toc-entry toc-h3"><a href="#CH1">CH1 </a></li>
<li class="toc-entry toc-h3"><a href="#AI">AI </a></li>
<li class="toc-entry toc-h3"><a href="#Representation-learning">Representation learning </a></li>
<li class="toc-entry toc-h3"><a href="#CH2">CH2 </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Deep-Generative-Modeling">Deep Generative Modeling </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Autoencoders:">Autoencoders: </a></li>
<li class="toc-entry toc-h3"><a href="#Variational-Autoencoders-(VAE)">Variational Autoencoders (VAE) </a></li>
<li class="toc-entry toc-h3"><a href="#Generative-Adversarial-Networks-(GANs)">Generative Adversarial Networks (GANs) </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-27-scribbles.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformers">
<a class="anchor" href="#Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers<a class="anchor-link" href="#Transformers"> </a>
</h2>
<p>To deal with sequential data we have to options:</p>
<ul>
<li>1-D convolution NN<ul>
<li>processing can be parallel </li>
<li>not practical for long sequences </li>
</ul>
</li>
<li>Recurrent NN <ul>
<li>can't happen in prallel </li>
<li>have gradient vanshing problem of the squence becomes so long</li>
<li>we have bottleneck at the end of the encoder </li>
</ul>
</li>
<li>RNN with attention mechanism <ul>
<li>to solve the bottleneck problem, we make Encoder-Decoder attention </li>
<li>Decoder utilzes:<ul>
<li>context vector </li>
<li>weighted sum of hidden states (h1,h2, ... ) from the encoder </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Transformers">
<a class="anchor" href="#Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers<a class="anchor-link" href="#Transformers"> </a>
</h3>
<h4 id="Encoder">
<a class="anchor" href="#Encoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Encoder<a class="anchor-link" href="#Encoder"> </a>
</h4>
<ul>
<li>first we do input embedding, and positional embedding </li>
<li>in self attention: we multiply q,w,v by a matrix to do lenear transformation</li>
<li>self attentoion: k <em> q --&gt; scaling down --&gt; softmax --&gt; </em> v </li>
</ul>
<h3 id="multi-head--attention">
<a class="anchor" href="#multi-head--attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>multi-head  attention<a class="anchor-link" href="#multi-head--attention"> </a>
</h3>
<ul>
<li>works as we use many filters in CNN  </li>
<li>in wide attention: it takes every word and spread it multi-head attention</li>
<li>in narrow attention:  it take every word and split it up across the multi-head <ul>
<li>but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?</li>
</ul>
</li>
</ul>
<h3 id="Positional-info">
<a class="anchor" href="#Positional-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>Positional info<a class="anchor-link" href="#Positional-info"> </a>
</h3>
<ul>
<li>positional encoding using the rotation sin/cos matrix </li>
<li>positional embedding </li>
</ul>
<h3 id="Residual-connections">
<a class="anchor" href="#Residual-connections" aria-hidden="true"><span class="octicon octicon-link"></span></a>Residual connections<a class="anchor-link" href="#Residual-connections"> </a>
</h3>
<ul>
<li>to give the chance to skip some learning parameters if it's better to minimize the loss </li>
</ul>
<h3 id="Layer-Normalization">
<a class="anchor" href="#Layer-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layer Normalization<a class="anchor-link" href="#Layer-Normalization"> </a>
</h3>
<ul>
<li>in batch normalization <ul>
<li>==&gt; we normalize to zero mean and unity varince </li>
<li>we calculate for all samples in each batch (for each channel )</li>
</ul>
</li>
<li>in layer normalization <ul>
<li>==&gt;  $y = \gamma * x +  \beta $  where gamm and bata are trainable parametes</li>
<li>calculates  for all  channles in the same sample </li>
</ul>
</li>
<li>in instance normalization ==&gt; calculate for one channel in one sample </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Debugging-ML-Models">
<a class="anchor" href="#Debugging-ML-Models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Debugging ML Models<a class="anchor-link" href="#Debugging-ML-Models"> </a>
</h2>
<ul>
<li>
<p>Understand bias-variance diagnoses</p>
<ul>
<li>getting more data ==&gt; fixes high variance </li>
<li>smaller set of features ==&gt; fixes high variance </li>
</ul>
</li>
</ul>
<h4 id="Refrence">
<a class="anchor" href="#Refrence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refrence<a class="anchor-link" href="#Refrence"> </a>
</h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=ORrStCArmP4">Prof. Andrew NG Vid</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SVM-and-Kernels">
<a class="anchor" href="#SVM-and-Kernels" aria-hidden="true"><span class="octicon octicon-link"></span></a>SVM and Kernels<a class="anchor-link" href="#SVM-and-Kernels"> </a>
</h2>
<ul>
<li>The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently </li>
</ul>
<p>we have $ X(i) \in R^{100}$, suppose  W can be expressed as a linear combintaion of X</p>
<p>$ W = \sum_{i = 1}^{M} \alpha_{i} y^i x^i$   (This can be proved with the representer theorem)</p>
<ul>
<li>vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left.</li>
</ul>
<p>optimization problem is :
$\min  {w,b}  {1/2} *||W||^2 $<br>
s.t  $y^i*((W^T * x^i) + b) &gt;= 1$</p>
<ul>
<li>For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Distributed-Training--in-Pytorch">
<a class="anchor" href="#Distributed-Training--in-Pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distributed Training  in Pytorch<a class="anchor-link" href="#Distributed-Training--in-Pytorch"> </a>
</h2>
<h3 id="Pytorch-DDP-Internal">
<a class="anchor" href="#Pytorch-DDP-Internal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pytorch DDP Internal<a class="anchor-link" href="#Pytorch-DDP-Internal"> </a>
</h3>
<p>DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP.
The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state.
DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.</p>
<p>Backward pass: Because <code>backward()</code> function is called on the loss directly, which out of DDP's control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization.</p>
<p>DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes.</p>
<p>Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.</p>
<h3 id="DataParallel-VS-DistributedDataParallel">
<a class="anchor" href="#DataParallel-VS-DistributedDataParallel" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataParallel VS DistributedDataParallel<a class="anchor-link" href="#DataParallel-VS-DistributedDataParallel"> </a>
</h3>
<ul>
<li>
<code>DataParallel</code> is single-process, multi-thread, and only works on a single machine, while <code>DistributedDataParallel</code> is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs.</li>
<li>
<code>DataParallel</code> doesn't support model parallel </li>
</ul>
<h4 id="Resources">
<a class="anchor" href="#Resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources<a class="anchor-link" href="#Resources"> </a>
</h4>
<pre><code>- https://pytorch.org/docs/master/notes/ddp.html
- https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Complete-Statistical-Theory-of-Learning---Vladimir-Vapnik">
<a class="anchor" href="#Complete-Statistical-Theory-of-Learning---Vladimir-Vapnik" aria-hidden="true"><span class="octicon octicon-link"></span></a>Complete Statistical Theory of Learning - <strong>Vladimir Vapnik</strong><a class="anchor-link" href="#Complete-Statistical-Theory-of-Learning---Vladimir-Vapnik"> </a>
</h2>
<ul>
<li>There are two ways for generalization: more data, and complete learning theory</li>
<li>Turing said, that you should imitate intelligent person </li>
</ul>
<h3 id="Ref:-https://www.youtube.com/watch?v=Ow25mjFjSmg">
<a class="anchor" href="#Ref:-https://www.youtube.com/watch?v=Ow25mjFjSmg" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ref: <a href="https://www.youtube.com/watch?v=Ow25mjFjSmg">https://www.youtube.com/watch?v=Ow25mjFjSmg</a><a class="anchor-link" href="#Ref:-https://www.youtube.com/watch?v=Ow25mjFjSmg"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Statistical-Machine-Learning">
<a class="anchor" href="#Statistical-Machine-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Statistical Machine Learning<a class="anchor-link" href="#Statistical-Machine-Learning"> </a>
</h2>
<h3 id="part-1:">
<a class="anchor" href="#part-1:" aria-hidden="true"><span class="octicon octicon-link"></span></a>part 1:<a class="anchor-link" href="#part-1:"> </a>
</h3>
<h4 id="deduction-vs-induction:">
<a class="anchor" href="#deduction-vs-induction:" aria-hidden="true"><span class="octicon octicon-link"></span></a>deduction vs induction:<a class="anchor-link" href="#deduction-vs-induction:"> </a>
</h4>
<ul>
<li>deduction: is the process of reasoning from one or more general statements to reach a logically certain conclusion. premises must be correct.</li>
<li>induction:  reasoning that construct or evaluates general proposition that are derived from specific examples. we can never be sure our conclusion can be wrong! </li>
<li>machine learning tries to automate the process of inductive inference.<br>
#### why should ML work?</li>
<li>ML tries to find patterns in data </li>
<li>we will only be able to learn if there's something to learn</li>
<li>ML makes some assumptions, which are are rarely made explicit.</li>
<li>we need to have an idea what we are looking for. This is called "inductive bias". Learning is impossible without such a bias<ul>
<li>the formal theorem if this is called <code>no free lunch theorem</code>
</li>
</ul>
</li>
<li>on the other hand, if we have a very strong inductive bias, then with just few training examples, then we can have high certainty in the output</li>
<li>the problem of selecting a good hypothesis class is called model selection.</li>
<li>any system that learns has an inductive bias.</li>
<li>if the algorithm works, <strong>THERE HAS TO BE A BIAS</strong>
</li>
<li>the inductive bias, rules over our function space 
### Part 2:</li>
<li>it;s not hard for ML algorithm to correctly predict training labels</li>
<li>usually ML algorithms make training errors, that is the function,they come up with doesn't perfectly fit the training data</li>
<li>what we care about is the performance on teh test set</li>
<li>it's not always the case that lowering the train data would lower that test data
#### K-Nearest algorithms:</li>
<li>for K-nearest algorithm, <ul>
<li>the best value for K is log(N)</li>
<li>if k is too  small ==&gt; overfitting </li>
<li>if k is too large ==&gt; underfitting</li>
</ul>
</li>
<li>k nearest algo achieves good results on MNIST dataset for classifying two classes, with simple euclidean distance function</li>
<li>k-nearest can be used for density estimation, clustering, outlier detection </li>
<li>the inductive bias in K nearest algo, is that near points are the of teh same category </li>
<li>the challenging part about k nearest algo is how to measure the distance between points</li>
</ul>
<h3 id="Part3">
<a class="anchor" href="#Part3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part3<a class="anchor-link" href="#Part3"> </a>
</h3>
<ul>
<li>for ML, we don't put any assmuptions for the data probability ditribution</li>
<li>often, the input and output are random variables </li>
<li>in some applications, it's important that the loss depends on the input X.</li>
<li>also in some cases, the type of error is critical, for exp. spaam detection </li>
<li>Bayes risk: is the min error for the expected values over all examples --&gt; basically the it's the lowest error you can achieve</li>
<li>Consistency: we say algorithm A is consistent, if we have an infinite iid datapoints, and the risk of algorithm's selected function converges to the Baye's risk. <ul>
<li>basically means that if we have infinite data samples, then our algorthms reaches the Bayes risk, which is the lowest error possible </li>
</ul>
</li>
<li>Universally consistent: no mattter the underlying probability distribution is, when we have enough data points, the algorithm would be consistent<ul>
<li>consistent independantly of the data distribution</li>
<li>KNN classifier, SVM, boositn, random forests are universally consistent</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="DL-Book">
<a class="anchor" href="#DL-Book" aria-hidden="true"><span class="octicon octicon-link"></span></a>DL Book<a class="anchor-link" href="#DL-Book"> </a>
</h2>
<h3 id="CH1">
<a class="anchor" href="#CH1" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH1<a class="anchor-link" href="#CH1"> </a>
</h3>
<ul>
<li>one of the key ideas in DL is that data representation matters a lot, and that DL is a technique for learning how to represent the data</li>
</ul>
<h3 id="AI">
<a class="anchor" href="#AI" aria-hidden="true"><span class="octicon octicon-link"></span></a>AI<a class="anchor-link" href="#AI"> </a>
</h3>
<ul>
<li>in AI we need the computer to do some tasks like humans</li>
<li>we do that by providing the computer with a lot of rules describing the world and how to act in different scenarios
### ML</li>
<li>in machine learning we can learn these rules without explicitly told them</li>
<li>but we still need to be provided with custom features that are given by domain experts </li>
</ul>
<h3 id="Representation-learning">
<a class="anchor" href="#Representation-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Representation learning<a class="anchor-link" href="#Representation-learning"> </a>
</h3>
<ul>
<li>a specific type of ML where we don't tell the computer the specific features</li>
<li>instead, we give the computer raw input, and it should learn the more complex features explicitly</li>
<li>ex: autoencoders 
### DL </li>
<li>is a representation learning algorithms that is applied in multi sequential manner</li>
</ul>
<p><img src="/Boda-Blog/images/copied_from_nb/assets/learning-paradigms.png" alt=""></p>
<h3 id="CH2">
<a class="anchor" href="#CH2" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH2<a class="anchor-link" href="#CH2"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-Generative-Modeling">
<a class="anchor" href="#Deep-Generative-Modeling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Generative Modeling<a class="anchor-link" href="#Deep-Generative-Modeling"> </a>
</h2>
<ul>
<li>latent variable: it's a variable that is controlling some behaviors, but we can't directly observe it</li>
<li>we are trying to observe <code>true explanatory factors</code>, for example, <code>latent variables</code>, from only observed data </li>
</ul>
<h3 id="Autoencoders:">
<a class="anchor" href="#Autoencoders:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Autoencoders:<a class="anchor-link" href="#Autoencoders:"> </a>
</h3>
<ul>
<li>the encoder learns to map from data, to a low-dimensional latent space </li>
<li>the decoder learns to map back from the low-dimensional space back into a reconstructed observation </li>
<li>the bottleneck hidden layer forces the network to learn a compressed latent representation</li>
<li>the reconstruction loss forces the latent representation to capture as much information from the data</li>
</ul>
<h3 id="Variational-Autoencoders-(VAE)">
<a class="anchor" href="#Variational-Autoencoders-(VAE)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variational Autoencoders (VAE)<a class="anchor-link" href="#Variational-Autoencoders-(VAE)"> </a>
</h3>
<ul>
<li>with classic autoencoders, once we train the network, then the latent representation is deterministic</li>
<li>but in VAE, we add some randomness, so we can generate new samples </li>
<li>so the encoder should output a mean and a standard deviation, which represents a distribution of the input, then we can sample from this distribution to generate new sample </li>
<li>the encoder is trying to infer a probability distribution of the latent space with respect to its input data</li>
<li>the decoder is trying to infer a new probability distribution over the input space given the latent space </li>
<li>the loss is going to be function of the parameters of the two distributions </li>
<li>the loss would consist of a construction loss and a regularization term, which is responsible for inducing some notion of structure of this probabilistic space </li>
<li>We need regularization and a prior to:<ul>
<li>continuity: points that are close in the latent space, should remain close after decoding</li>
<li>completeness: samples from the latent space, should produce meaning content after decoding </li>
</ul>
</li>
<li>we can't perform back propagation, as there's stochasticity in the latent space,</li>
<li>
<p>to solve this issue, we fix, the mean and variance, and introduce the stochastic term separate from them</p>
</li>
<li>
<p>The key problem with VAEs, is a concern of density estimation</p>
</li>
</ul>
<h3 id="Generative-Adversarial-Networks-(GANs)">
<a class="anchor" href="#Generative-Adversarial-Networks-(GANs)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Generative Adversarial Networks (GANs)<a class="anchor-link" href="#Generative-Adversarial-Networks-(GANs)"> </a>
</h3>
<ul>
<li>we need to sample from a very complex distribution, that we don't know, and can't estimate</li>
<li>the solution, is to sample from a simple distribution (eg. noise), then learn a transformation, to the data distribution  </li>
<li>we have a Generator, that's tries to transform the data sampled from the random noise, into data that looks real, to trick the discriminator</li>
<li>we have a discriminator, which tries to identify real data from fake. </li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="BodaSadalla98/Boda-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Boda-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Boda-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is an unorganized posts in whih I try to summraize my readings as a way to help remember the knowlegde</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/BodaSadalla98" target="_blank" title="BodaSadalla98"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/bodasadala" target="_blank" title="bodasadala"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
