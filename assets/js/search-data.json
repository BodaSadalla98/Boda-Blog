{
  
    
        "post0": {
            "title": "Univnet",
            "content": "TTS (Text To Speech) . TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. . Wavenet . Wavenet V1 . before wavenet, ther was two methods: . generative method: which would produce the over all song of the sentece well, but would fail to produce the individual sounds well | . | concatinative: . we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence | . wavenet: . tries to do both of the above methods, it also can change the speaker by changing some parameters | . | data output: 16 khz rate . | we cant use normal RNN as the max seq length around 50 . | they used dilated CNNs: . can have very long look back | fast to train | . | . WaveNet: is a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals.WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. . Wavenet V2 . The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn&#39;t depend on previous samples to produce the current sample, while still maintaining the same quality. . WaveGan . WaveGAN is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms). . The WaveGAN architecture is based off DCGAN. The DCGAN generator uses the transposed convolution operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed convolution operation to widen its receptive field, using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5, and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way, using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters, numerical operations, and output dimensionality as DCGAN . Before WaveGan . Autoregressive generation: . It&#39;s an approach in which speech samples are generated one by one in sequence. | Examples: WaveNet | Has high quality | Takes around 180 secs to generate a one second of speech | can&#39;t be applied to services in production due to low speed | . Non-autoregressive generation: . It&#39;s an approach where all voice samples are generated in prallel | Examples: Prallel WaveNet | Lower quality than autoregressive method | takes 0.03 seconds to generates one second of speed | . End-to-End TTS . End-to-end TTS systems can be splitted into two main components: . Speech Synthesizer, which takes in raw text and output mel-spectrogram. Ex: Tacotron | . | Vocoder, which takes in mel-spectrogram and outputs sound waves. Ex: Prallel WaveGan, Univnet | . | . Tacotron2 . Tacotron 2 is a neural network architecture for speech synthesis directly from text. It consists of two components: a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. . . Prallel WaveGan . Parallel WaveGAN1, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system. . . Univnet . UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multiresolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input . . Resources . Wavenet https://deepmind.com/blog/article/high-fidelity-speech-synthesis-wavenet | https://www.youtube.com/watch?v=YyUXG-BfDbE | https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html | https://deepmind.com/blog/article/wavenet-generative-model-raw-audio | . | WaveGan https://arxiv.org/pdf/1802.04208v3.pdf | https://paperswithcode.com/method/wavegan | . | Prallel WaveGan . https://www.youtube.com/watch?v=knzT7M6qsl0 | https://github.com/kan-bayashi/ParallelWaveGAN | https://arxiv.org/pdf/1910.11480.pdf | . | Tacotron . https://arxiv.org/pdf/1712.05884v2.pdf | . | Univnet https://arxiv.org/pdf/2106.07889.pdf | . | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/tts/2022/02/13/Univnet.html",
            "relUrl": "/jupyter/deeplearning/python/tts/2022/02/13/Univnet.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Applied Deep Learning",
            "content": "Deep Learning overview . we can look at deep learning as an algorithm that writes algorithms, like a compiler . in this case the source code would be the data: (examples/experiences) | excutable code would be the deployable model . | Deep: Functions compositions $ f_l f_{l-1} .... f_1$ . | Learning: Loss, Back-propagation, and Gradient Descent . | $ L( theta) approx J( theta)$ --&gt; noisy estimate of the objective function due to mini-batching. That&#39;s why we call it stochastic Gradient Descent . | why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it&#39;s N*N, so it&#39;s computationally expensive and slow ### Optimizers | to make gradient descent faster, we can add momentum to it. | another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum | . | . RMSprop: A mini-batch version of rprop method. the original rprop can&#39;t work with mini batches, as it doesn&#39;t consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. . . | . Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there | . Adam: can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta) | can also has momentum for all parameter wich can lead to faster convergence | . | Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal | . ImageNet: . Dropout . A simple method to prevent the NN from overfitting | CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image | you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. | . Network In Network . the main idea is to put a network inside another network . | they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers . | this idea is bisacally a (one to one convution) | they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels from the last conv layer to form the output layer | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html",
            "relUrl": "/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "NLP Specialization",
            "content": "Course 1 . Weak 4 . Hashing . We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space | . Locality senstive hashing . the idea is to put items that are close in the vector space, in the same hashing buckets | we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly . | but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can&#39;t be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words . | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/nlp/2022/01/30/Coursera_NLP_Specialization.html",
            "relUrl": "/jupyter/deeplearning/python/nlp/2022/01/30/Coursera_NLP_Specialization.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "CS480/680",
            "content": "Lecture 12 . Gausain process . infinite dimentional gaussian distribution | . Lecture 16 . Convolution NN . a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters | . Residual Networks . even after using Relu, NN can still suffer from gradient vanishing | the idea in to add skip connections so that we can create shorter paths | . Lecture 18 . LSTM vs GRU vs Attention . LSTM: 3 gates, one for the cell state, one for the input, one for the output | GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state . takes less parameters | . | Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token . | . Lecture 20 . Autoencoder . takes different input and generates the same output . used in: . compression | denoising | sparse representation | data generation | . Lecture 21 . Generative models . Variational autoencoders . idea: train the encoder to sample a fixed distribution , | we want the network to sample a fixed distribution that is close to the distribution of the encoder, so that it generate similar outputs to the input, but not the same | . GANS: . - . Lecture 22 . Ensemble Learning: . the idea is to combine the hypothesis of several models to produce a better one #### Bagging choose the class the majority votes #### Weighted majority decrease the weight of corrlelated hypothesis | increase the weight of good hypothesis #### Boosting | . | . | . - idea: when an instance is missclassified by hypothesis, increase its weight so that the next hypothesis is more likely to classify it correctly - can boost a weak learner - makes weighted training set, so that it can focus on missclassified examples - at the end generate weighted hypotheses based on the acc of each hypothsis - Advantages: - no need to learn perfect hypothesis - can boost any weak learning algo - boosting is very simple - has good generalization . Netflex challenge 2006 . Lecture 23 . Normalizing flows . Lecture 24 . Gradient boosting . boosting for regression | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deep%20learning/2021/12/14/CS480-680-Intro-to-Machine-Learning.html",
            "relUrl": "/jupyter/deep%20learning/2021/12/14/CS480-680-Intro-to-Machine-Learning.html",
            "date": " • Dec 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Voice Conversion",
            "content": "AutoVC . paper | Demo | Repo | . Speach Split . Paper | Demo | repo | . AutoPST . Paper | Demo | repo | . Sources . IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING | Voice Conversion Challenge 2020 results | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/voiceconversion/2021/11/21/VoiceConversion.html",
            "relUrl": "/jupyter/deeplearning/voiceconversion/2021/11/21/VoiceConversion.html",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Python Notebook",
            "content": "Inheritance . If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn&#39;t overload it | . Ex: . class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don&#39;t override the constructor and use the parent one . Multiple Inheritance . when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority . if the two parent classes have the same function, then Method Resolution Orded (MRO) makes the fist class method to be the one called | . | if we override a function from the parent class in the child class, the cild class has the pariority . | to call the both finctions, then we can call the parent class from the child class directly . ex: in the child class we can do: parent.method() | . | . Underscore . single underscore (Before): used for internal variables | double undescore (After): used also for internal scopes only, and also tells python to change the variable name(mangling) | underscore (After): helps avoid conflicts with key words ex(class, int, etc) . | underscore (Before and Afer): used for thing like __init, main__, etc . | . Decorators . def function(func): def inner(): print(1) func() print(3) return inner @function def print_name(): print(2) print_name() . 1 2 3 . so in this example we decorated a function (print_name), which means we gonna pass the deocared function as a paramemter to the decorator | so with decorator, we call our function in the inner function, the decorate it, then return the decorated function | used when u wanna decorate a multiple function with the same, suppose you have an add and multiply function, and you want to print something at the beginig of them, so you crate a decorator that does that, and decorate the two functions with that. | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/python/2021/11/10/Python.html",
            "relUrl": "/jupyter/python/2021/11/10/Python.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Scribbles Notebook",
            "content": "Transformers . To deal with sequential data we have to options: . 1-D convolution NN processing can be parallel | not practical for long sequences | . | Recurrent NN can&#39;t happen in prallel | have gradient vanshing problem of the squence becomes so long | we have bottleneck at the end of the encoder | . | RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention | Decoder utilzes: context vector | weighted sum of hidden states (h1,h2, ... ) from the encoder | . | . | . Transformers . Encoder . first we do input embedding, and positional embedding | in self attention: we multiply q,w,v by a matrix to do lenear transformation | self attentoion: k q --&gt; scaling down --&gt; softmax --&gt; v | . multi-head attention . works as we use many filters in CNN | in wide attention: it takes every word and spread it multi-head attention | in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN? | . | . Positional info . positional encoding using the rotation sin/cos matrix | positional embedding | . Residual connections . to give the chance to skip some learning parameters if it&#39;s better to minimize the loss | . Layer Normalization . in batch normalization ==&gt; we normalize to zero mean and unity varince | we calculate for all samples in each batch (for each channel ) | . | in layer normalization ==&gt; $y = gamma * x + beta $ where gamm and bata are trainable parametes | calculates for all channles in the same sample | . | in instance normalization ==&gt; calculate for one channel in one sample | . Debugging ML Models . Understand bias-variance diagnoses . getting more data ==&gt; fixes high variance | smaller set of features ==&gt; fixes high variance | . | . Refrence . Prof. Andrew NG Vid | . SVM and Kernels . The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently | . we have $ X(i) in R^{100}$, suppose W can be expressed as a linear combintaion of X . $ W = sum_{i = 1}^{M} alpha_{i} y^i x^i$ (This can be proved with the representer theorem) . vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left. | . optimization problem is : $ min {w,b} {1/2} *||W||^2 $ s.t $y^i*((W^T * x^i) + b) &gt;= 1$ . For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant | . Distributed Training in Pytorch . Pytorch DDP Internal . DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP. The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready. . Backward pass: Because backward() function is called on the loss directly, which out of DDP&#39;s control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization. . DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes. . Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration. . DataParallel VS DistributedDataParallel . DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs. | DataParallel doesn&#39;t support model parallel | . Resources . - https://pytorch.org/docs/master/notes/ddp.html - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2021/10/27/scribbles.html",
            "relUrl": "/jupyter/deeplearning/python/2021/10/27/scribbles.html",
            "date": " • Oct 27, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "TTS Research",
            "content": "TTS . TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. . End-to-end TTS . easier pipeline | better peformance | . Wavenet . Based on: . DeepMind&#39;s Wavenet | https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html | https://deepmind.com/blog/article/wavenet-generative-model-raw-audio | . Wavenet V1 . before wavenet, ther was two methods: . generative method, which would produce the over all song of the sentece well, but would fail to produce the individual sounds well | concatinative: we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence . | wavenet: tries to do both of the above methods, it also can change the speaker by changing some parameters . | data output: 16 khz rate . | we cant use normal RNN as the max seq length around 50 . | they used dilated CNNs: . can have very long look back | fast to train | . | . This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. . Wavenet V2 . The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn&#39;t depend on previous samples to produce the current sample, while still maintaining the same quality . End-to-end adversarial TTS . Based on: . https://www.youtube.com/watch?v=WTB2p4bqtXU | https://deepmind.com/research/publications/2020/End-to-End-Adversarial-Text-to-Speech | https://arxiv.org/abs/2006.03575 | . Adversarial: means we have a generator and a descriminator which tries to detect which output is generated and which is real End-to-end : they take in text and output the speech . steps: . we enter the text, then we tokenize it | then we use a stack of dilated conv layers to predict the length of each token | with this info, we can predict the center of each token | then we use a gaussian kernel to give a prob distributaion for the place of the token center | then the generator can generate the sound | . ** Q: here we assumed that every token starts directly after the one before it, but is that the case? don&#39;t we need to add a small duration of no sound between tokens? . Tacotron2 . based on: . paper | repo | . you train tacotron-like seq2seq model to output a mel spectrogram, then pass that to wavenet to generate the wave form . This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech. . Common TTS Architictures: . - Autoregressive models - DCCN - Flow - Teacher Student - Variational auto encoders (VAE) - GANS . Resources . https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2 | .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/tts/deeplearning/2021/10/14/Text-To-Speech.html",
            "relUrl": "/tts/deeplearning/2021/10/14/Text-To-Speech.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "The Science of Well Being",
            "content": "Misconcepions about happiness . Why our expectations are so bad . How can we overcome our biases . What&#39;s really makes us happy . Putting strategies into practice .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/2021/10/06/The-Science-of-Well-Being.html",
            "relUrl": "/2021/10/06/The-Science-of-Well-Being.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "FastAi 2020",
            "content": "Lecture two . P value: . determines if some numbers have realationship, or they are random (whether they are independat or dependant) suppose we have the temp and R (transmitity) values of a 100 cities in China and we want to see if there&#39;s a relation between them. then we generate many sets of random numbers for each parameter then we calculate the P value which would tell us what&#39;s the percentage this slope is a random, and that ther&#39;s no relation A P-value is the probability of an observed result assuming that the null hypothesis (there&#39;s no relation ) is true PS: P-value also is dependant on the size of the set u used, so they don&#39;t measure the importance of the result. so don&#39;t use P-values If the P value is &gt; 0.5 then we sure that these daata have no ralation, and if the p-value is so small, then there&#39;s a chance that the data have a relation . Lecture three . In the course video and book, we built a bear classifier, using data from Microsoft Ping Api. . To build a deep learning model, we have first to gather the data, then we should prepare the data to be in the right format for the model, then we train the model and observe if we get satisfiable results, if not then we try to investigate to try to get better results. Finally we save our model and deploy it! . while gathering the data, notice that all the time the data would be biased, and in sometimes these biases would be severe that they can&#39;t be ignored . Race classifier . I have tried to rebuild the notebook and to make a Race classifier. . I got the dataset from here Dataset, and then trained a small Resnet18 NN to classify images. . To deploy the app, I used Voila and MyBinder to make it available online here: Race Classifier . Lastly, all the code can be found in this githun repo here .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/fastai/2021/10/06/Fastai-2020.html",
            "relUrl": "/jupyter/fastai/2021/10/06/Fastai-2020.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Lecture 1",
            "content": "This is Post for Computational Linear Algebra . import numpy as np . a = np.array( [[6,5,3,1], [3,6,2,2], [3,4,3,1] ]) b = np.array( [ [1.5 ,1], [2,2.5], [5 ,4.5] ,[16 ,17] ]) . for c in (a @ b): print(c) . [50. 49.] [58.5 61. ] [43.5 43.5] . Lecture 2 . Matrix decomposition: we decopose matricies into smaller ones that has special properties . Singular Value Decomposition (SVD): . it&#39;s an exact decomposition, so you can retrieve the orginal matrix again | . Some SVD applications: . semantic analysis | collaborative filtering / recommendation | data compression | PCA (principal component analysis) | . Non-negative Matrix Factorization (NMF) .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/2021/10/02/Computational-Linear-Algebra.html",
            "relUrl": "/2021/10/02/Computational-Linear-Algebra.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . Running cells with &#39;Python 3.9.4 64-bit&#39; requires ipykernel package. Run the following command to install &#39;ipykernel&#39; into the Python environment. Command: &#39;/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall&#39; . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bodasadalla98.github.io/Boda-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bodasadalla98.github.io/Boda-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bodasadalla98.github.io/Boda-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bodasadalla98.github.io/Boda-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}