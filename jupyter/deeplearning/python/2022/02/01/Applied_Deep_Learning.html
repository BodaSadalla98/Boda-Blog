<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Applied Deep Learning | Boda Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Applied Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Applied Deep Learning Course" />
<meta property="og:description" content="Applied Deep Learning Course" />
<link rel="canonical" href="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" />
<meta property="og:url" content="https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" />
<meta property="og:site_name" content="Boda Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2022-02-01T00:00:00-06:00","url":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html","@type":"BlogPosting","headline":"Applied Deep Learning","dateModified":"2022-02-01T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bodasadalla98.github.io/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html"},"description":"Applied Deep Learning Course","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Boda-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bodasadalla98.github.io/Boda-Blog/feed.xml" title="Boda Blog" /><link rel="shortcut icon" type="image/x-icon" href="/Boda-Blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Boda-Blog/">Boda Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Boda-Blog/about/">About Me</a><a class="page-link" href="/Boda-Blog/search/">Search</a><a class="page-link" href="/Boda-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Applied Deep Learning</h1><p class="page-description">Applied Deep Learning <a href='https://github.com/maziarraissi/Applied-Deep-Learning'>Course</a></p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Boda-Blog/categories/#python">python</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/BodaSadalla98/Boda-Blog/tree/master/_notebooks/2022-02-01-Applied_Deep_Learning.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Boda-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/BodaSadalla98/Boda-Blog/master?filepath=_notebooks%2F2022-02-01-Applied_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/BodaSadalla98/Boda-Blog/blob/master/_notebooks/2022-02-01-Applied_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Boda-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Deep-Learning-overview">Deep Learning overview </a></li>
<li class="toc-entry toc-h2"><a href="#Dropout">Dropout </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Network-In-Network">Network In Network </a></li>
<li class="toc-entry toc-h3"><a href="#VGG-Net">VGG Net </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Local-Response-Normalization:">Local Response Normalization: </a></li>
<li class="toc-entry toc-h4"><a href="#Data-Augmentation">Data Augmentation </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#GoogleNet">GoogleNet </a></li>
<li class="toc-entry toc-h3"><a href="#Batch-Normalization">Batch Normalization </a>
<ul>
<li class="toc-entry toc-h4"><a href="#conv-layers">conv layers </a></li>
<li class="toc-entry toc-h4"><a href="#Benifits-of-batch-norm:">Benifits of batch norm: </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Parametric-Relu:">Parametric Relu: </a></li>
<li class="toc-entry toc-h3"><a href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):">Kaiming Initialization  (I didn&#39;t fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations): </a></li>
<li class="toc-entry toc-h3"><a href="#Label-smoothing-regularization">Label smoothing regularization </a></li>
<li class="toc-entry toc-h3"><a href="#ResNet">ResNet </a></li>
<li class="toc-entry toc-h3"><a href="#Identity-mapping-in-resnets">Identity mapping in resnets </a></li>
<li class="toc-entry toc-h3"><a href="#Wide-Residual-Networks">Wide Residual Networks </a></li>
<li class="toc-entry toc-h3"><a href="#ResNext">ResNext </a></li>
<li class="toc-entry toc-h3"><a href="#Squeeze-and-Ecxcitation-Networks">Squeeze-and-Ecxcitation Networks </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Squeeze-:-just-a-global-averaging-step">Squeeze : just a global averaging step </a></li>
<li class="toc-entry toc-h4"><a href="#Excitation:-is-just-a-fully-connected-newtwork">Excitation: is just a fully connected newtwork </a></li>
<li class="toc-entry toc-h4"><a href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention">Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Spatial-Transformer-Network">Spatial Transformer Network </a></li>
<li class="toc-entry toc-h3"><a href="#Dynamic-Routing-between-capsuls">Dynamic Routing between capsuls </a></li>
<li class="toc-entry toc-h3"><a href="#Knowledge-Distillation">Knowledge Distillation </a></li>
<li class="toc-entry toc-h3"><a href="#Network-Pruning:">Network Pruning: </a></li>
<li class="toc-entry toc-h3"><a href="#Quantization">Quantization </a></li>
<li class="toc-entry toc-h3"><a href="#Huffman-Coding">Huffman Coding </a></li>
<li class="toc-entry toc-h3"><a href="#Squeeze-Net">Squeeze Net </a></li>
<li class="toc-entry toc-h3"><a href="#XNOR-NET">XNOR-NET </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-01-Applied_Deep_Learning.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-Learning-overview">
<a class="anchor" href="#Deep-Learning-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning overview<a class="anchor-link" href="#Deep-Learning-overview"> </a>
</h2>
<ul>
<li>
<p>we can look at deep learning as an algorithm that writes algorithms, like a compiler</p>
<ul>
<li>in this case the source code would be the data: (examples/experiences)</li>
<li>
<p>excutable code would be the deployable model</p>
</li>
<li>
<p>Deep: Functions compositions  $ f_l f_{l-1} .... f_1$</p>
</li>
<li>
<p>Learning: Loss, Back-propagation, and Gradient Descent</p>
</li>
<li>
<p>$ L(\theta) \approx J(\theta)$ --&gt; noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent</p>
</li>
<li>why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow 
### Optimizers</li>
<li>to make gradient descent faster, we can add momentum to it.</li>
<li>another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p>RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign.</p>
<p><img src="assets/Applied_deep_learning/rprop.png" alt=""></p>
</li>
</ul>
<ul>
<li>Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Adam:<ul>
<li>can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)</li>
<li>can also has momentum for all parameter wich can lead to faster convergence</li>
</ul>
</li>
<li>Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dropout">
<a class="anchor" href="#Dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout<a class="anchor-link" href="#Dropout"> </a>
</h2>
<ul>
<li>A simple method to prevent the NN from overfitting </li>
<li>CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image </li>
<li>you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Network-In-Network">
<a class="anchor" href="#Network-In-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network In Network<a class="anchor-link" href="#Network-In-Network"> </a>
</h3>
<ul>
<li>
<p>the main idea is to put a network inside another network</p>
</li>
<li>
<p>they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers</p>
</li>
<li>this idea is bisacally a (one to one convution) </li>
<li>they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>one by one convolution is a normal convolution with fliter size of 1 by 1 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had  slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant)</li>
<li>we can achieve local invariant with pooling, and deal with global invariant with data augmentation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="VGG-Net">
<a class="anchor" href="#VGG-Net" aria-hidden="true"><span class="octicon octicon-link"></span></a>VGG Net<a class="anchor-link" href="#VGG-Net"> </a>
</h3>
<h4 id="Local-Response-Normalization:">
<a class="anchor" href="#Local-Response-Normalization:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Response Normalization:<a class="anchor-link" href="#Local-Response-Normalization:"> </a>
</h4>
<ul>
<li>
<p>the idea is to normalize a pixel across nearing channels</p>
</li>
<li>
<p>after comparing nets with lrn and nets without, they didn't find big difference, so they stoped using it</p>
</li>
</ul>
<h4 id="Data-Augmentation">
<a class="anchor" href="#Data-Augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Augmentation<a class="anchor-link" href="#Data-Augmentation"> </a>
</h4>
<ul>
<li>Image translations( random crops), and horizontal reflection </li>
<li>altering the intensities of the RGB channels </li>
<li>scale jittering   </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="GoogleNet">
<a class="anchor" href="#GoogleNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>GoogleNet<a class="anchor-link" href="#GoogleNet"> </a>
</h3>
<ul>
<li>You stack multiple inception modules on top of each ohter </li>
<li>the idea is that you don't have to choose which filter size to use, so why don't use them all </li>
<li>to make the network more efficient, they first projected the input with one by one convolution then applied the main filters </li>
<li>you concatinate the many filters through the channel dimension    </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Batch-Normalization">
<a class="anchor" href="#Batch-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization<a class="anchor-link" href="#Batch-Normalization"> </a>
</h3>
<ul>
<li>The main goal of batch normalization is to redude the <code>Internal Covariant Shift</code>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>we can just normalize the inputs and it would work fine</li>
<li>the problem is that in each following layer, and statistics of its output would depend on its weights </li>
<li>so we also need to nomalize the inputs in hidden layers </li>
<li>
<p>here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen</p>
</li>
<li>
<p>in inference we can't have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset</p>
</li>
</ul>
<h4 id="conv-layers">
<a class="anchor" href="#conv-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>conv layers<a class="anchor-link" href="#conv-layers"> </a>
</h4>
<ul>
<li>for conv layers we apply normalization across every channel for every pixel in the batch of images</li>
<li>the effective bach size would be ==&gt; m<em>p</em>q where m is the number of images in the batch and 
p,q are the image resolution </li>
</ul>
<h4 id="Benifits-of-batch-norm:">
<a class="anchor" href="#Benifits-of-batch-norm:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Benifits of batch norm:<a class="anchor-link" href="#Benifits-of-batch-norm:"> </a>
</h4>
<ul>
<li>you can use higher learning rate, as the training is more stable </li>
<li>less sensitive to initialization </li>
<li>less sensitive to activation function </li>
<li>it has regularization effects, because thre's random mini batch every time </li>
<li>preserve gradient magintude ?? maybe --&gt; because the jacobian doesn't scale as we scales the weights </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parametric-Relu:">
<a class="anchor" href="#Parametric-Relu:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parametric Relu:<a class="anchor-link" href="#Parametric-Relu:"> </a>
</h3>
<p>$ f({y_i}) = \max(0,y_i) + a_i \min(0, y_i) $</p>
<ul>
<li>if $a_i = 0$  --&gt; Relu</li>
<li>if $a_i = 0.01$ --&gt; Leaky Relu</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>the initialization of weights and biases depends on the type of activation function </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):">
<a class="anchor" href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kaiming Initialization  (I didn't fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations):<a class="anchor-link" href="#Kaiming-Initialization--(I-didn't-fully-understand-the-heavy-math-in-this-lecture,-as-Im-still-weak-in-statistics-and-variance-calculations):"> </a>
</h3>
<ul>
<li>professor went into deep mathematical details into how to choose the intial values for weights</li>
<li>the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we  want the weights to have values centred around 1 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Label-smoothing-regularization">
<a class="anchor" href="#Label-smoothing-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Label smoothing regularization<a class="anchor-link" href="#Label-smoothing-regularization"> </a>
</h3>
<ul>
<li>the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ResNet">
<a class="anchor" href="#ResNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNet<a class="anchor-link" href="#ResNet"> </a>
</h3>
<ul>
<li>The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection.</li>
</ul>
<h3 id="Identity-mapping-in-resnets">
<a class="anchor" href="#Identity-mapping-in-resnets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identity mapping in resnets<a class="anchor-link" href="#Identity-mapping-in-resnets"> </a>
</h3>
<ul>
<li>the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Wide-Residual-Networks">
<a class="anchor" href="#Wide-Residual-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wide Residual Networks<a class="anchor-link" href="#Wide-Residual-Networks"> </a>
</h3>
<ul>
<li>an attempt to make resnets wider and study if that would make them better </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ResNext">
<a class="anchor" href="#ResNext" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNext<a class="anchor-link" href="#ResNext"> </a>
</h3>
<ul>
<li>just like resnets but they changed bottleneck blocks with group convolution block </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Squeeze-and-Ecxcitation-Networks">
<a class="anchor" href="#Squeeze-and-Ecxcitation-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze-and-Ecxcitation Networks<a class="anchor-link" href="#Squeeze-and-Ecxcitation-Networks"> </a>
</h3>
<h4 id="Squeeze-:-just-a-global-averaging-step">
<a class="anchor" href="#Squeeze-:-just-a-global-averaging-step" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze : just a global averaging step<a class="anchor-link" href="#Squeeze-:-just-a-global-averaging-step"> </a>
</h4>
<h4 id="Excitation:-is-just-a-fully-connected-newtwork">
<a class="anchor" href="#Excitation:-is-just-a-fully-connected-newtwork" aria-hidden="true"><span class="octicon octicon-link"></span></a>Excitation: is just a fully connected newtwork<a class="anchor-link" href="#Excitation:-is-just-a-fully-connected-newtwork"> </a>
</h4>
<h4 id="Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention">
<a class="anchor" href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention<a class="anchor-link" href="#Scaling-:-multiply-every-channel-with-the-corresponding-exctitiaiton-value,-more-like-attention"> </a>
</h4>
<ul>
<li>scaling is you paying different attention to different channels like attention models </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Spatial-Transformer-Network">
<a class="anchor" href="#Spatial-Transformer-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spatial Transformer Network<a class="anchor-link" href="#Spatial-Transformer-Network"> </a>
</h3>
<ul>
<li>
<p>the main idea is to seperate the main object in the image, like putting a box around it and then this box can be resized, shifted, rotated. so in the end we have a focused image that has only the object, and so we can apply convolution on it and it would be easy then</p>
</li>
<li>
<p>the idea is to first find a good transformation parameters theta, you can do that using NN</p>
</li>
<li>then for every position in the output image, you do a bilinear sampling from the input image</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dynamic-Routing-between-capsuls">
<a class="anchor" href="#Dynamic-Routing-between-capsuls" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dynamic Routing between capsuls<a class="anchor-link" href="#Dynamic-Routing-between-capsuls"> </a>
</h3>
<ul>
<li>the idea is to make the outputs of the capsule has a norm that is the probability that an object is presenet</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Knowledge-Distillation">
<a class="anchor" href="#Knowledge-Distillation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Knowledge Distillation<a class="anchor-link" href="#Knowledge-Distillation"> </a>
</h3>
<ul>
<li>the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter <code>T</code> that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Network-Pruning:">
<a class="anchor" href="#Network-Pruning:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network Pruning:<a class="anchor-link" href="#Network-Pruning:"> </a>
</h3>
<ul>
<li>all connections with weights below a threshold are removed from the network </li>
<li>weight are sparse now </li>
<li>then we can represent them using fewer bits</li>
</ul>
<h3 id="Quantization">
<a class="anchor" href="#Quantization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quantization<a class="anchor-link" href="#Quantization"> </a>
</h3>
<ul>
<li>we basically cluster our weight to some centroids</li>
<li>the number of centroids for conv layers are more than the ones for FC layers why:<ul>
<li>because conv layer filters are already sparse, we need higher level of accuracy in them</li>
<li>FC layers are so dense that we can tolerate fewer quantization levels </li>
</ul>
</li>
</ul>
<h3 id="Huffman-Coding">
<a class="anchor" href="#Huffman-Coding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Huffman Coding<a class="anchor-link" href="#Huffman-Coding"> </a>
</h3>
<ul>
<li>store the more common symbols with more bits </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Squeeze-Net">
<a class="anchor" href="#Squeeze-Net" aria-hidden="true"><span class="octicon octicon-link"></span></a>Squeeze Net<a class="anchor-link" href="#Squeeze-Net"> </a>
</h3>
<ul>
<li>the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made </li>
<li>the main idea  is to use one by one comvultion to reduce the dimensionality</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="XNOR-NET">
<a class="anchor" href="#XNOR-NET" aria-hidden="true"><span class="octicon octicon-link"></span></a>XNOR-NET<a class="anchor-link" href="#XNOR-NET"> </a>
</h3>
<ul>
<li>the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation</li>
<li>the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==&gt; $W = \alpha * B $ where alpha  is postative 32 bit constant and B is a binary matrix </li>
<li>
<p>then mean we try to train by using a means square error loss function of the original weights and alpha and B</p>
</li>
<li>
<p>I still can't fully understand  how to binarize the input</p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="BodaSadalla98/Boda-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Boda-Blog/jupyter/deeplearning/python/2022/02/01/Applied_Deep_Learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Boda-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Boda-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is an unorganized posts in whih I try to summraize my readings as a way to help remember the knowlegde</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/BodaSadalla98" target="_blank" title="BodaSadalla98"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/bodasadala" target="_blank" title="bodasadala"><svg class="svg-icon grey"><use xlink:href="/Boda-Blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
