{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81bd892",
   "metadata": {},
   "source": [
    "# Scribbles Notebook \n",
    "> A bunch of very different topics scribbles \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d030bf",
   "metadata": {},
   "source": [
    "## Transformers \n",
    "\n",
    "To deal with sequential data we have to options:\n",
    "   - 1-D convolution NN\n",
    "       - processing can be parallel \n",
    "       - not practical for long sequences \n",
    "   - Recurrent NN \n",
    "       - can't happen in prallel \n",
    "       - have gradient vanshing problem of the squence becomes so long\n",
    "       - we have bottleneck at the end of the encoder \n",
    "   - RNN with attention mechanism \n",
    "       - to solve the bottleneck problem, we make Encoder-Decoder attention \n",
    "       - Decoder utilzes:\n",
    "           - context vector \n",
    "           - weighted sum of hidden states (h1,h2, ... ) from the encoder \n",
    "\n",
    "### Transformers \n",
    "\n",
    "#### Encoder\n",
    "- first we do input embedding, and positional embedding \n",
    "- in self attention: we multiply q,w,v by a matrix to do lenear transformation\n",
    "- self attentoion: k * q --> scaling down --> softmax --> * v \n",
    " \n",
    "### multi-head  attention \n",
    "- works as we use many filters in CNN  \n",
    "- in wide attention: it takes every word and spread it multi-head attention\n",
    "- in narrow attention:  it take every word and split it up across the multi-head \n",
    "    - but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?\n",
    "\n",
    "### Positional info\n",
    "\n",
    "- positional encoding using the rotation sin/cos matrix \n",
    "- positional embedding \n",
    "\n",
    "### Residual connections\n",
    "- to give the chance to skip some learning parameters if it's better to minimize the loss \n",
    "\n",
    "### Layer Normalization \n",
    "\n",
    "- in batch normalization \n",
    "    - ==> we normalize to zero mean and unity varince \n",
    "    - we calculate for all samples in each batch (for each channel )\n",
    "- in layer normalization \n",
    "    - ==>  $y = \\gamma * x +  \\beta $  where gamm and bata are trainable parametes\n",
    "    - calculates  for all  channles in the same sample \n",
    "- in instance normalization ==> calculate for one channel in one sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b18421",
   "metadata": {},
   "source": [
    "## Debugging ML Models \n",
    "\n",
    "- Understand bias-variance diagnoses\n",
    "\n",
    "    - getting more data ==> fixes high variance \n",
    "    - smaller set of features ==> fixes high variance \n",
    "    \n",
    "\n",
    "#### Refrence\n",
    "   - [Prof. Andrew NG Vid](https://www.youtube.com/watch?v=ORrStCArmP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28478c",
   "metadata": {},
   "source": [
    "## SVM and Kernels\n",
    "\n",
    "* The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently \n",
    "\n",
    "we have $ X(i) \\in R^{100}$, suppose  W can be expressed as a linear combintaion of X\n",
    "\n",
    "$ W = \\sum_{i = 1}^{M} \\alpha_{i} y^i x^i$   (This can be proved with the representer theorem)\n",
    "- vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left.\n",
    "\n",
    "optimization problem is :\n",
    "$\\min  {w,b}  {1/2} *||W||^2 $  \n",
    "s.t  $y^i*((W^T * x^i) + b) >= 1$\n",
    "\n",
    "* For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5db2ab",
   "metadata": {},
   "source": [
    "## Distributed Training  in Pytorch\n",
    "\n",
    "### Pytorch DDP Internal \n",
    "\n",
    "DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP.\n",
    "The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state.\n",
    "DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.\n",
    "\n",
    "Backward pass: Because `backward()` function is called on the loss directly, which out of DDP's control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization.\n",
    "\n",
    "DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes.\n",
    "\n",
    "Optimizer Step: From the optimizerâ€™s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.\n",
    "\n",
    "\n",
    "### DataParallel VS DistributedDataParallel\n",
    "\n",
    "- `DataParallel` is single-process, multi-thread, and only works on a single machine, while `DistributedDataParallel` is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs.\n",
    "- `DataParallel` doesn't support model parallel \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d18e86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677f2c2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df610005",
   "metadata": {},
   "source": [
    "\n",
    "#### Resources \n",
    "    - https://pytorch.org/docs/master/notes/ddp.html\n",
    "    - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25536f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
