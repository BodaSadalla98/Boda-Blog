{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222c0e35",
   "metadata": {},
   "source": [
    "# Scribbles Notebook \n",
    "> A bunch of very different topics scribbles \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e3a34",
   "metadata": {},
   "source": [
    "## Transformers \n",
    "\n",
    "To deal with sequential data we have to options:\n",
    "   - 1-D convolution NN\n",
    "       - processing can be parallel \n",
    "       - not practical for long sequences \n",
    "   - Recurrent NN \n",
    "       - can't happen in prallel \n",
    "       - have gradient vanshing problem of the squence becomes so long\n",
    "       - we have bottleneck at the end of the encoder \n",
    "   - RNN with attention mechanism \n",
    "       - to solve the bottleneck problem, we make Encoder-Decoder attention \n",
    "       - Decoder utilzes:\n",
    "           - context vector \n",
    "           - weighted sum of hidden states (h1,h2, ... ) from the encoder \n",
    "\n",
    "### Transformers \n",
    "\n",
    "#### Encoder\n",
    "- first we do input embedding, and positional embedding \n",
    "- in self attention: we multiply q,w,v by a matrix to do lenear transformation\n",
    "- self attentoion: k * q --> scaling down --> softmax --> * v \n",
    " \n",
    "### multi-head  attention \n",
    "- works as we use many filters in CNN  \n",
    "- in wide attention: it takes every word and spread it multi-head attention\n",
    "- in narrow attention:  it take every word and split it up across the multi-head \n",
    "    - but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?\n",
    "\n",
    "### Positional info\n",
    "\n",
    "- positional encoding using the rotation sin/cos matrix \n",
    "- positional embedding \n",
    "\n",
    "### Residual connections\n",
    "- to give the chance to skip some learning parameters if it's better to minimize the loss \n",
    "\n",
    "### Layer Normalization \n",
    "\n",
    "- in batch normalization \n",
    "    - ==> we normalize to zero mean and unity varince \n",
    "    - we calculate for all samples in each batch (for each channel )\n",
    "- in layer normalization \n",
    "    - ==>  $y = \\gamma * x +  \\beta $  where gamm and bata are trainable parametes\n",
    "    - calculates  for all  channles in the same sample \n",
    "- in instance normalization ==> calculate for one channel in one sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646ed87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d1d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
